{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd94d36-9062-45c0-a95a-38a5143a660e",
   "metadata": {},
   "source": [
    "# Application 2: New models\n",
    "Now it's time for you to come up with promising solutions for our classification task. \n",
    "\n",
    "Generally, the following approaches came to my mind:\n",
    "1. Try to improve the current model\n",
    "    * Find better hyper parameters / introduce so far neglected ones\n",
    "    * Modify the classification head\n",
    "    * Optimize our normalization procedure and/or test out different configurations (like removing emojis rather than replacing them with their name, which is the default behaviour)\n",
    "2. Use a larger pre-trained model or different BERT variant\n",
    "    * Find a larger version of the model for example [DeBERTa small](https://huggingface.co/microsoft/deberta-v3-base).\n",
    "    * Find a similar deep learning model like vanilla BERT for example. Generally, [Hugging Face](https://huggingface.co/models) offers a wide selection of pre-trained models ([preferrably in the category `Text Classification`](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads)) and is rather easy to cutomize for your application.\n",
    "3. Use a different machine learning technique\n",
    "    * Gradient boosting provides great results quickly also on text classification tasks. \n",
    "        * Very easy to get some quick results ([see this example](https://suatatan.com/posts/sklearn_xgboost_tc/)).   \n",
    "        (See notebook `scripts/new_models_implementations/gradient_boosting_catboost_classifier.ipynb`  for an example)\n",
    "        * [CatBoost](https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier) even allows for including embeddings as features for training (check out [this tutorial](https://towardsdatascience.com/boosted-embeddings-with-catboost-8dc15e18fb9a)).    \n",
    "            (See notebook `scripts/new_models_implementations/gradient_boosting_xgb_classifier.ipynb` for an example)\n",
    "    * User-defined shallow LSTM with pre-trained [embeddings](https://nlp.stanford.edu/projects/glove/) (see [this example](https://towardsdatascience.com/pre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead)). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a22848-c77f-40fe-aecb-27ba36755a48",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "* Pick your favorite approach and try to implement it. Feel free to add your personal ideas to the list and implement them. Share them with the group!  \n",
    "\n",
    "  *Hint*, start out with a subsample of the dataset when exploring new models as you will likely run into bugs....\n",
    "* Analyze your results and compare your performance to the baseline model.\n",
    "* If you have the time, go back to the first Task and repeat :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b6561-26d7-4dc3-8649-b84701714c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
