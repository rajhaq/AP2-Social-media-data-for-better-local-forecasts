{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aeb90d0-89e3-42aa-82b8-a43860dbd0d8",
   "metadata": {},
   "source": [
    "# Application 2: Train our baseline model (DeBERTa)\n",
    "\n",
    "Finally, it is time to train our model and solve our classification task. We will \n",
    "1. train our classifier based on the [DeBERTa small](https://huggingface.co/microsoft/deberta-v3-small/tree/main) model.\n",
    "2. visualize our results in form of confusion matrix, roc curve and certainty of classifier. \n",
    "\n",
    "The model is trained on GPUs. Thus, you need to make sure that you run this notebook with GPU support. For this, create a new [JupyterLabs session](https://jupyter-jsc.fz-juelich.de/hub/home) and pick a `Partition` with GPU support like \"gpus\" or \"develgpus\". 1 Node will suffice for training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd5f94-cca3-4076-8fa3-0cc43f0ca3bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Exercise 0: Gain a basic understanding of BERT and its derivatives\n",
    "Generally, our problem can be phrased as a text classification task, where deep learning models based on the transformer architecture (Vaswani et al. 2017) achieve state-of-the-art results (e.g., Yang et al. 2019). Transformers use self-attention to attribute varying relevance to different parts of the text. Based on transformers the BERT model was developed (Devlin et al. 2019), which includes crucial pre-training steps to familiarise the model with relevant vocabulary and semantics. In addition, the DeBERTa model (He et al. 2021a) adds additional pre-training steps and disentangles how positional and text information is stored in the model to improve performance. For this experiment we rely on the most recent version of the model, which is DeBERTaV3 (He et al. 2021b).\n",
    "We use the default version of DeBERTaV3_small as described in He et al. 2021b with an adopted head for text classification. For this, we add a dropout layer with user-specified dropout rate and a pooling layer that passes the embedding of the special initial token ([CLS], comprising the meaning of the whole Tweet) to the loss function.\n",
    "\n",
    "To gain a deeper insight into \"self-attention\", BERT and DeBERTa, I provide you with a presentation discussing the project, which goes into great detail to explain these concepts. The slide can be found [here](https://docs.google.com/presentation/d/1hW4_wLj0QIEhFQdKqVj6Uep2fWMqrbeek_5IjOUXeCQ/edit?usp=sharing) but I will also give the presentation at the beginning of the session. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28534677-a0ea-4f8a-863e-06b31c59896f",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "* Ask at least one \"dumb\" and one \"smart\" question during the presentation :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b59e2-64d7-40a7-9951-a19a05824886",
   "metadata": {},
   "source": [
    "## Exercise 1: Package overview\n",
    "We will use the [Hugging Face](https://huggingface.co/docs/transformers/index) for training our DeBERTa model. The library contains a plethora of [pre-trained models](https://huggingface.co/models) that can easily be [accessed](https://huggingface.co/docs/transformers/autoclass_tutorial) (including tokenization), modified for your task (used for classification, next sentence predicition, etc.) and [fine-tuned](https://huggingface.co/docs/transformers/training) for your use case. \n",
    "\n",
    "For reference, the saved pretrained weights of the \"small\" DeBERTa version can be found [here](https://huggingface.co/microsoft/deberta-v3-small) (which corresponds to our best model currently) and the model documentation/ implementation details [here](https://huggingface.co/docs/transformers/model_doc/deberta-v2).\n",
    "\n",
    "However, we will go through setting up the model together step by step. Let's start by importing all relevant packages. Besides modules from the Hugging Face library. We are importing modules from [scikit-learn](https://scikit-learn.org/stable/), which is a popular library that we use for splitting our dataset and analyze our results. In addition, we use [PyTorch](https://pytorch.org/), which is low leve (compared to Hugging Face) library that can be used to build deep learning models from scratch. Here, we just use it to test if the notebook has access to gpus and to compute some metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2181bb8d-6109-4de9-aeb1-c4943d14ae10",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "* Execute the following cell to import all required packages.  \n",
    "  If you encounter any errors, make sure you are using the correct ipython kernel. If you cannot fix the issue yourself, do not \"waste\" your time and quickly approach a tutor for help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4a89d-0b31-4e08-838a-786df3eeae5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c030e60-5942-47e9-aba4-95e1169e6576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950d702-cf6c-4362-bb3e-7efa581a80ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# may take some time ...\n",
    "import sys\n",
    "import pathlib\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import functools\n",
    "\n",
    "# location of scripts folder for bootcamp\n",
    "sys.path.append(\"../scripts\")\n",
    "import normalize_text_bootcamp\n",
    "import utils_bootcamp\n",
    "import plotting\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray\n",
    "\n",
    "# Pytorch modules\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "# scikit-learn modules\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "\n",
    "# \"Hugging Face\" modules\n",
    "import datasets\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf987dc-9b30-40b3-997b-1e35e23557c3",
   "metadata": {},
   "source": [
    "## Exercise 2: Small intro to using and monitoring GPUs on HPC\n",
    "We will now take a look at how to \n",
    "1. check if our training functions can \"see\" GPUs. For this and the next task we will use pytorch \n",
    "2. reset the GPU memory. This can be useful when you use the same module to train another version of the model. As the memory may not automatically reset especially when you encounter a crash while training....\n",
    "3. monitor GPU recources like processing power and memory. Functionality for this is built into our jupyter environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07205fbd-6b48-41ed-bcc6-e0cda150283e",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "* Use `[torch.cuda.device(i) for i in range(torch.cuda.device_count())]` to check if PyTorch and therefore the Hugging Face library, which we will use for training has access to your GPUs. You should see 4 devices (a node contains 4 GPUs). \n",
    "\n",
    "    If it returns an empty list, make sure you picked a `Partition` that includes GPUs when setting up JupyterLabs. If the problem persists, again try to approach a tutor as this can \"waste\" considerable amount of your time and access to GPUs is crucial for the rest of the week.\n",
    "* To reset the memory use `torch.cuda.empty_cache()`. Not much is gonna change visibly but the next task is gonna change that.\n",
    "* JupyterLabs comes by default with monitoring tools. They can be found in the left toolbar. Click on the fourth symbol from the top called `System Dashboards`. A menu should open, where you should now double-click `GPU Utilization`. This which should open up a new tab in your JupyterLabs session, which shows the load on your GPUs, which should basically be zero as we are currently not employing any GPUs. Patience, may be required when opening these tools as they can take some time to load and update. Go back to the menu and also open `GPU Memory`, which gives an overview of allocated GPU memory. This can help you debug your code when you run into memory issues. Finally, we open `GPU Resources`, which provides you with a concise overview of GPU utilization, memory and communication. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3128d4-d037-46d6-ae75-925309b67bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[torch.cuda.device(i) for i in range(torch.cuda.device_count())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03f94a-406e-45ea-a29c-d97d56c7ef95",
   "metadata": {},
   "source": [
    "## Exercise 3: Loading and splitting the data\n",
    "We load the data and split the data into a training set, which we use for training the model and a test set, which is set a side during training and used to analyze the efficacy of the model. This procedure comes with the benefit that we automatically control for overfitting.  We use `sklearn.model_selection.train_test_split` to split the dataset. However, we will not literally split the dataset in two but rather use two sets of indices `indices_train` and `indices_test` to later select the desired part of the dataset for either training or analyzing, respectively. We would like to 'stratify' our train and test set, which allows us to uphold the same fraction of labels in the test and training set, which reduces biases. \n",
    "\n",
    "## Tasks:\n",
    "* Load the SMALL dataset (with filtered and normalized Tweets) (saved as '/p/project/training2223/a2/data/tweets/tweets_2017_era5_normalized_filtered.nc') with xarray and assign it to variable `ds_raw`.\n",
    "* Obtain indices for the training (80% of dataset) and test set (20% of dataset), while stratifying the sets based on our label (presence of rain at time/location of Tweet). Use the code fragment below to get started and have a look at the [documentation of the function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). \n",
    "* Plot a histogram of the distribution of the labels in the training and test set. Do they follow the same distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b707c3-d938-481f-b5dc-9231b8d72cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ds_tweets = ...\n",
    "FOLDER_DATA = \"/p/project/deepacf/maelstrom/haque1/dataset/\"\n",
    "FOLDER_TWEET = FOLDER_DATA + \"tweets_2017_01_era5_normed_filtered.nc\"\n",
    "ds_tweets = xarray.load_dataset(FOLDER_TWEET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e38d1-66aa-4e5b-91b9-7a3d55cc5e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# again define labels\n",
    "key_tp = \"tp_h\"\n",
    "ds_tweets[\"raining\"] = ([\"index\"], ds_tweets[key_tp].values > 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dbdf8c-11b0-4189-b3e4-f82e3e7d8447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_train, indices_test = sklearn.model_selection.train_test_split(\n",
    "    np.arange(ds_tweets[\"index\"].shape[0]),\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    test_size=0.2,\n",
    "    stratify=ds_tweets[\"raining\"].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd8e6b6-40cf-416b-ad48-c87ea605f0ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_train, indices_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e5206-19ca-49f7-9aed-940e072c4d4f",
   "metadata": {},
   "source": [
    "## Exercise 4: Load the tokenizer and bring the data into the expected format by Hugging Face\n",
    "We will load the pretrained tokenizer that will be used to convert our Tweets to 'arrays of numbers' and introduce a dictionary to store the dataset to be usable by the Hugging Face training function. The following tasks are quite guided as there are more exciting parts of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafcdb45-98f5-472d-b945-5ae6b71918c9",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "\n",
    "* Prepare our dataset for training by tokenizing Tweets and converting into the expected data format. Just executing the following lines should do the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524136f4-efc9-451b-a879-9bfdb26e1df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the pretrained tokenizer\n",
    "model_nm = (\n",
    "    \"/p/project/deepacf/maelstrom/haque1/deberta-v3-small\"  # model repo downloaded from Hugging Face to the cluster\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_nm)\n",
    "db_config_base = transformers.AutoConfig.from_pretrained(model_nm, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c44db-6752-4234-af5f-350a2df0148a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define functions to convert the dataset to a format that is used by Hugging Face\n",
    "\n",
    "\n",
    "def tok_func(x, tokenizer):\n",
    "    \"\"\"\n",
    "    tokenizes the field 'inputs' stored in x including padding\n",
    "    \"\"\"\n",
    "    return tokenizer(x[\"inputs\"], padding=True)\n",
    "\n",
    "\n",
    "def get_dataset(ds, tok_func, tokenizer, indices_train, indices_test, train=True):\n",
    "    \"\"\"\n",
    "    converts dataset to 'dataset' format required by Hugging Face\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    ds: dataset\n",
    "    tok_func: functiond use for tokenization\n",
    "    indices_train: indices corresponding to the training set\n",
    "    indices_test: indices corresponding to the training set\n",
    "    train: if used for training\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    header of file\n",
    "    \"\"\"\n",
    "    # converting dataset to pandas as Hugging Face datasets has inbuilt function that converts pandas dataframe to a Hugging Face dataset\n",
    "    df = ds[[\"text_normalized\", \"raining\"]].to_pandas()\n",
    "    df = df.rename(columns={\"text_normalized\": \"inputs\"})\n",
    "    df = df.rename(columns={\"raining\": \"label\"})\n",
    "    datasets_ds = datasets.Dataset.from_pandas(df)\n",
    "    tok_function_partial = functools.partial(tok_func, tokenizer=tokenizer)\n",
    "    tok_ds = datasets_ds.map(tok_function_partial, batched=True)\n",
    "    if train:\n",
    "        return datasets.DatasetDict({\"train\": tok_ds.select(indices_train), \"test\": tok_ds.select(indices_test)})\n",
    "    else:\n",
    "        return tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cefecd-ed46-4457-a326-5ac21774ebb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Hugging Face 'dataset'\n",
    "dataset = get_dataset(ds_tweets, tok_func, tokenizer, indices_train, indices_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c00552-a654-4b9b-87be-3c7b833a7309",
   "metadata": {},
   "source": [
    "## Exercise 5: Define parameters and functions used for actual training step \n",
    "\n",
    "In the following we will define our default values for our hyper parameters. Some of these parameters are model independent and others specific to the model. While documentation for model independent parameters can found [here](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments), model dependant paramters can be found on the [dedicated page](https://huggingface.co/docs/transformers/model_doc/deberta#transformers.DebertaConfig), which can be a bit confusing.\n",
    "\n",
    "* learning_rate = 8e-5\n",
    "* batch_size = 16\n",
    "* weight_decay = 0.01\n",
    "* epochs = 1\n",
    "* warmup_ratio = 0.1\n",
    "* cls_dropout = 0.3\n",
    "* lr_scheduler_type = \"cosine\"\n",
    "\n",
    "Task:\n",
    "* Define your personal folder where Hugging Face should save trained weights and debugging information as `FOLDER_TO_OUTPUT`. This folder tends to be quite large and may overwrite results of your colleaques, so make sure it's in your personal directory.\n",
    "* Execute cells below and try to get a first impression of what these functions are doing. For example, by reading their docstring.\n",
    "* We are using the f1 score as our main metric. How is it computed and why is considered more 'useful' than accuracy for example? We compute the f1-score of 'raining' and not 'raining', which one is more instructive for model performance? Let's discuss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7844e7d-478c-466d-98e1-2e4915f79fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we define our hyper-parameters in a dictionary `parameters`\n",
    "parameters = {}\n",
    "parameters[\"learning_rate\"] = 8e-5\n",
    "parameters[\"batch_size\"] = 16\n",
    "parameters[\"weight_decay\"] = 0.01\n",
    "parameters[\"epochs\"] = 1\n",
    "parameters[\"warmup_ratio\"] = 0.1\n",
    "parameters[\"cls_dropout\"] = 0.3\n",
    "parameters[\"lr_scheduler_type\"] = \"cosine\"\n",
    "\n",
    "FOLDER_TO_OUTPUT = \"/p/project/deepacf/maelstrom/haque1/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f3a19-111b-4400-af5f-8e044f6e3074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(params, db_config_base, model_nm):\n",
    "    \"\"\"\n",
    "    function to retrieve model, format follows Hugging Face convention (parameter -> 'params')\n",
    "    \"\"\"\n",
    "    db_config = db_config_base\n",
    "    if params is not None:\n",
    "        db_config.update({\"cls_dropout\": params[\"cls_dropout\"]})\n",
    "    db_config.update({\"num_labels\": 2})\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(model_nm, config=db_config)\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    compute f1 metrics of both labels, format follows Hugging Face convention\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    eval_pred: evaluation/test set probalities for classification task\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary returning labeled f1 score of \"not raining\" and \"raining\"\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    classification_report = sklearn.metrics.classification_report(\n",
    "        labels, predictions, target_names=[\"not raining\", \"raining\"], output_dict=True\n",
    "    )\n",
    "    f1_not_raining = classification_report[\"not raining\"][\"f1-score\"]\n",
    "    f1_raining = classification_report[\"raining\"][\"f1-score\"]\n",
    "    return {\"f1_not_raining\": f1_not_raining, \"f1_raining\": f1_raining}\n",
    "\n",
    "\n",
    "def get_trainer(dataset, db_config_base, model_nm, FOLDER_TO_OUTPUT, parameters):\n",
    "    \"\"\"\n",
    "    initializes `transformers.Trainer`, which is used to train models with Hugging Face\n",
    "\n",
    "    Hyper parameters are here assigned to model.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataset: dataset in format required by Hugging Face\n",
    "    db_config_base: default model configurations\n",
    "    model_nm: model folder\n",
    "    FOLDER_TO_OUTPUT: folder where trained model, tokenizer,... will be saved\n",
    "    parameters: dictionary of hyper-parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    trainer with assigned parameters used for training\n",
    "    \"\"\"\n",
    "    args = transformers.TrainingArguments(\n",
    "        FOLDER_TO_OUTPUT,\n",
    "        learning_rate=parameters[\"learning_rate\"],\n",
    "        warmup_ratio=parameters[\"warmup_ratio\"],\n",
    "        lr_scheduler_type=parameters[\"lr_scheduler_type\"],\n",
    "        disable_tqdm=False,\n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=parameters[\"batch_size\"],\n",
    "        per_device_eval_batch_size=parameters[\"batch_size\"],\n",
    "        num_train_epochs=parameters[\"epochs\"],\n",
    "        weight_decay=parameters[\"weight_decay\"],\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    # convert `get_model` to partial function to pass it as an argument in `transformers.Trainer`\n",
    "    # see https://www.geeksforgeeks.org/partial-functions-python/ for quick tutorial\n",
    "    get_model_partial = functools.partial(get_model, db_config_base=db_config_base, model_nm=model_nm)\n",
    "    return transformers.Trainer(\n",
    "        model_init=get_model_partial,\n",
    "        args=args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4be94b0-88d8-405d-aa5d-52f227ac3d88",
   "metadata": {},
   "source": [
    "## Exercise 6: Train your first DeBERTa model \n",
    "\n",
    "Now, it's time to train the model. Just call the `train` method and enjoy. Note, we are initializing the model with pre-trained weights. The model should therefore have an understanding of 'text'. To fine-tune the model for our specific use-case, we are only training for a single epoch to accustom it to predicting rain from our Tweets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd5f0c5-8293-44d6-bcea-899ce581ee49",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "* Train the model by executing the following lines of code.\n",
    "* What are your results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e90f35-54f6-4bfe-b60f-7409b46946d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = get_trainer(dataset, db_config_base, model_nm, FOLDER_TO_OUTPUT, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151c6e8-1778-49d3-9765-fa7298f7f69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7668250-6dbb-40f7-ad2d-a58742c6e0b3",
   "metadata": {},
   "source": [
    "## Exercise 7: Analyze results \n",
    "\n",
    "To improve our models and gain a deeper understanding on their characteristics, we would like to look at additional metrics other than the f1 score. And find ways to visualize our results to get a more detailed view on our predictions than a single number (f1-score) can provide.\n",
    "   \n",
    "For this, we would like to plot a confusion matrix, a ROC curve and an inidicator to evaluate the confidence of the model. To visualize any results, we will first need to make predictions with our model on the test set, which is done in the next cell. \n",
    "\n",
    "**Confusion matrix:**\n",
    "Now, we can plot a confusion matrix, which can easily be done by using [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and their provided function   \n",
    "`sklearn.metrics.confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)`\n",
    "* `y_true`: true label values\n",
    "* `y_pred`: predicted label values\n",
    "* `labels`: names of our labels (\"not raining\", \"raining\")\n",
    "* `normalize`: allows normalization of matrix by default number of labels is shown. we use `normalize='all'` such that total fractions add up to 1.\n",
    "The function `sklearn.metrics.ConfusionMatrixDisplay` provides a way to plot the resulting confusion matrix. This is all implemented in function `plotting.analysis.check_prediction`, which you can use for the task.\n",
    "\n",
    "**AUC and ROC:**     \n",
    "After obtaining your sigmoid probabilities at the end of your classification task. You are left with two values, one for either label. However, usually you assume the threshold to be at 0.5, however you can always shift the threshold of what's considered labeled as \"raining\" and \"not raining\" to change for example your True Positive Rate or your False Positive Rate. Therefore to have a more 'objective' measure of the performance of your classifier, ROC and AUC come in handy as they consider all thresholds. If you are unfamiliar with the concept of a ROC curve (receiver operating characteristic curve) and the derived AUC value (Area Under the ROC Curve), take a look at [google's explanation](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc). \n",
    "\n",
    "**Model confidence in predictions:**     \n",
    "To check the confidence of our model in its predictions we bin Tweets according to the predicted probability (after the softmax layer) of the two labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede79be-b70f-45ac-b4ed-7a2167bb0605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is the test dataset in the format expected by Hugging Face\n",
    "test_ds = get_dataset(\n",
    "    ds_tweets.sel(index=indices_test),\n",
    "    tok_func,\n",
    "    tokenizer,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    "    train=False,  # not training anymore\n",
    ")\n",
    "# this is a selection of our xarray dataset that corresponds to the tweets that are part of the test set\n",
    "ds_test = ds_tweets.sel(index=indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5dfa88-e1a6-4dd7-990b-40a2284aa5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = torch.nn.functional.softmax(torch.Tensor(trainer.predict(test_ds).predictions)).numpy()\n",
    "prediction_probability = preds[:, 1]\n",
    "predictions = preds.argmax(axis=-1)\n",
    "truth = ds_test.raining.values\n",
    "plotting.analysis.classification_report(labels=truth, predictions=predictions)\n",
    "plotting.analysis.plot_roc(truth=truth, prediction_probability=prediction_probability)\n",
    "plotting.plotting.analysis.check_prediction(truth=truth, prediction=predictions);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b683d34a-32c4-4e68-8c39-6c90fb6325b6",
   "metadata": {},
   "source": [
    "As the Hugging Face Classifier class returns predictions after the [logit](https://www.google.com/search?q=logit&client=firefox-b-d&ei=8-IqY5WMGc-Axc8Px9izkAs&ved=0ahUKEwjV0fqr0qX6AhVPQPEDHUfsDLIQ4dUDCA0&uact=5&oq=logit&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEJECMgUIABCRAjILCC4QgAQQxwEQ0QMyBQgAEIAEMgUIABCABDIFCAAQgAQyBQgAEIAEMgUIABCABDIFCAAQgAQyBQgAEIAEOgoIABBHENYEELADOgcIABCwAxBDOgQIABBHSgQIQRgASgQIRhgAULIDWPEGYJULaAFwAngAgAF-iAHOAZIBAzEuMZgBAKABAcgBCMABAQ&sclient=gws-wiz) function, we need to apply a [sigmoid](https://www.google.com/search?q=sigmoid+function&client=firefox-b-d&ei=6-MqY9yuIuqHxc8Pz8-ggAQ&ved=0ahUKEwic0KSi06X6AhXqQ_EDHc8nCEAQ4dUDCA0&uact=5&oq=sigmoid+function&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEJECMgUIABCRAjIFCAAQkQIyBQgAEJECMgUIABCABDIFCAAQkQIyBQgAEIAEMgUIABCABDIFCAAQgAQyBQgAEIAEOgoIABBHENYEELADOgcIABCwAxBDOgQIABBDOgsILhCABBDHARDRAzoFCC4QgAQ6CAguEIAEENQCSgQIQRgASgQIRhgAUMIEWOAPYKMQaAJwAXgAgAF2iAHACZIBBDEzLjKYAQCgAQHIAQrAAQE&sclient=gws-wiz) function (binary classification) layer or [softmax](https://www.google.com/search?q=softmax+function&client=firefox-b-d&ei=q-cqY6qzNJySxc8Pu9CNqAs&ved=0ahUKEwjqtJjs1qX6AhUcSfEDHTtoA7UQ4dUDCA0&uact=5&oq=softmax+function&gs_lcp=Cgdnd3Mtd2l6EAMyBAgAEEMyBAgAEEMyBAgAEEMyBAgAEEMyBAgAEEMyBQgAEIAEMgUIABCABDIFCAAQgAQyBQgAEIAEMgUIABCABDoKCAAQRxDWBBCwAzoHCAAQsAMQQzoHCAAQgAQQCjoICAAQHhAWEAo6BQgAEIYDOgQIABAKOgQIABANSgQIQRgASgQIRhgAUJYWWIw7YNg7aAxwAXgAgAFoiAHMDJIBBDE5LjGYAQCgAQHIAQrAAQE&sclient=gws-wiz) for multi-class classification, which corresponds to a generalized form of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35e8c0-eff2-4be6-9a85-02d4c80f5860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = torch.nn.functional.softmax(torch.Tensor(trainer.predict(test_ds).predictions)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ff94c-68f4-4644-8808-ef66d5f4d043",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "In the following tasks, we will compare our predictions to the ground truth values, which corresponds to our 'raining' field in `ds_tweets`, i.e. `ds_tweets.raining.values`. This field is two-dimensional. However, our predictions are now two dimensional as the sigmoid function converts our predictions into probabilities for either class (\"raining\", \"not raining\"). Thus, we obtain our \"1D\" prediction by using `numpy.argmax`, which returns the index of the maximum value along the axis. We apply this function over the last axis to obtain our 'predictions', i.e. `prediction=preds.argmax(-1)`.\n",
    "* Plot a confusion matrix of your model's result (use `plotting.analysis.check_prediction` for this).   \n",
    "    *Note*, using `print(report)` on the returned `report` gives you a nice representation than the default Jupyter Notebook output.\n",
    "* Plot a ROC curve and compute the AUC.     \n",
    "    *Hint*, to compute ROC and AUC, we need the prediction probability for our label, i.e. `prediction_probability = preds[:,1]`. \n",
    "* Visualize the confidence of the model in its predictions. Use `plotting.analysis.plotting.analysis.plot_predictions_confidence` for this task.\n",
    "* Retrain the model by changing one or more hyper-parameters as defined in Exercise 5. Change your model output folder `FOLDER_TO_OUTPUT` to load weights later and compare models. To inspire your parameters choice. I list the 'best' hyper-parameters we currently use for our models.\n",
    "    * learning_rate = 3e-05\n",
    "    * batch_size = 32\n",
    "    * weight_decay = 0.01\n",
    "    * epochs = 1\n",
    "    * warmup_ratio = 0.45\n",
    "    * cls_dropout = 0.1\n",
    "    * lr_scheduler_type = 'linear'   \n",
    "    \n",
    "    As an additional challenge, **only change** hyper-parameters if you know what they do. Share your results in class! \n",
    "* Visualize your new results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759fd7bf-ff7e-4612-8994-a6e7853b3fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_FOLDER = \"checkpoint-2983\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9af51-dd94-4671-a112-059988f31ee9",
   "metadata": {},
   "source": [
    "Note, if you just want to evaluate one of your trained models without any further training. You just need to change the model path `model_nm` to the desired checkpoint in your output folder of your saved model `FOLDER_TO_OUTPUT` (Check the contents of the `FOLDER_TO_OUTPUT` to check the specific path). We provide a function here, that combines all required functions into a single function `load_saved_trained_model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae130c-8d48-4180-8bdb-6c5556d1165d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if loading required of saved model\n",
    "def load_saved_trained_model(ds, folder_to_model, db_config_base, model_nm, parameters):\n",
    "    # load the pretrained tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(folder_to_model)\n",
    "    db_config_base = transformers.AutoConfig.from_pretrained(folder_to_model, num_labels=2)\n",
    "    dataset = get_dataset(ds, tok_func, tokenizer, indices_train, indices_test)\n",
    "    trainer = get_trainer(dataset, db_config_base, folder_to_model, folder_to_model, parameters)\n",
    "    return trainer\n",
    "\n",
    "\n",
    "trainer_evaluate = load_saved_trained_model(\n",
    "    ds_tweets,\n",
    "    FOLDER_TO_OUTPUT + CHECKPOINT_FOLDER,\n",
    "    db_config_base,\n",
    "    model_nm,\n",
    "    parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafda7cc-c810-4fb5-9080-b8b4dbde28b6",
   "metadata": {},
   "source": [
    "## Exercise 8: Retrain on full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd6029-ed44-4b88-80d6-71838e71c338",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "* Retrain your best model on the full dataset\n",
    "* Did you improve your results?\n",
    "* Analyze the results with the introduced methods in this notebook. Share your results with the group!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1949e8d-2674-400a-a9a5-3b129a1f6a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5d0e2-69af-4994-bead-976374b85f93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap2",
   "language": "python",
   "name": "ap2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
