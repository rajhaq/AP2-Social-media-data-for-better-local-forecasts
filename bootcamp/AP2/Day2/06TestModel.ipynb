{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b53a5-711c-45ce-b06d-c290a4b73e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146e902-4454-40e1-be0f-bde77704ceb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c1e2d-cd21-4bc2-be42-b991eb31df78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import pathlib\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import functools\n",
    "\n",
    "# Add the scripts folder to the system path\n",
    "sys.path.append(\"../scripts\")\n",
    "import normalize_text_bootcamp\n",
    "import utils_bootcamp\n",
    "import plotting\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Import data handling and visualization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray\n",
    "\n",
    "# PyTorch for deep learning\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "# Scikit-learn for data processing and metrics\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "\n",
    "# Hugging Face for DeBERTa model\n",
    "import datasets\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7a42f-ef89-4328-b128-a6d75631176d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/p/project/deepacf/maelstrom/haque1/dataset/tweets_2017_01_era5_normed_filtered.nc\"\n",
    "ds_tweets = xarray.load_dataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514fe644-5ad6-49d7-8bb1-f70a835161df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc605e5a-4ea3-47c2-be58-9249c44a069a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_tp = \"tp_h\"\n",
    "ds_tweets[\"raining\"] = ([\"index\"], ds_tweets[key_tp].values > 1e-8)\n",
    "\n",
    "indices_train, indices_test = sklearn.model_selection.train_test_split(\n",
    "    np.arange(ds_tweets[\"index\"].shape[0]),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=ds_tweets[\"raining\"].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5832b-adb7-4248-a5aa-4dd8739b89a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be893f5a-6b81-425e-8173-f9b5b092dbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.hist(indices_train, bins=np.unique(indices_train), alpha=0.5, label='Training Set')\n",
    "# plt.hist(indices_test, bins=np.unique(indices_test), alpha=0.5, label='Test Set')\n",
    "# plt.xlabel('Label')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Label Distribution in Training and Test Set')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7507311-6ecc-4550-bf1f-bb4d076c4e11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pretrained tokenizer\n",
    "model_nm = \"/p/project/deepacf/maelstrom/haque1/deberta-v3-small\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_nm)\n",
    "db_config_base = transformers.AutoConfig.from_pretrained(model_nm, num_labels=2)\n",
    "\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"inputs\"], padding=True, truncation=True)\n",
    "\n",
    "\n",
    "# Prepare the dataset for the Hugging Face model\n",
    "def get_dataset(ds, tokenizer, indices_train, indices_test):\n",
    "    df = ds[[\"text_normalized\", \"raining\"]].to_dataframe()\n",
    "    df = df.rename(columns={\"text_normalized\": \"inputs\", \"raining\": \"label\"})\n",
    "    datasets_ds = datasets.Dataset.from_pandas(df)\n",
    "    tok_ds = datasets_ds.map(tokenize_function, batched=True)\n",
    "    train_dataset = tok_ds.select(indices_train)\n",
    "    test_dataset = tok_ds.select(indices_test)\n",
    "    return datasets.DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = get_dataset(ds_tweets, tokenizer, indices_train, indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f58d8-ef0f-4013-b024-f27bfc67fe0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "parameters = {\n",
    "    \"learning_rate\": 8e-5,\n",
    "    \"batch_size\": 16,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"epochs\": 1,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"cls_dropout\": 0.3,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "}\n",
    "\n",
    "# Specify your personal folder for model outputs\n",
    "FOLDER_TO_OUTPUT = \"/p/project/deepacf/maelstrom/your_user/model/\"\n",
    "\n",
    "\n",
    "# Function to get the model\n",
    "def get_model(params, db_config_base, model_nm):\n",
    "    db_config = db_config_base\n",
    "    if params is not None:\n",
    "        db_config.update({\"cls_dropout\": params[\"cls_dropout\"]})\n",
    "    db_config.update({\"num_labels\": 2})\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(model_nm, config=db_config)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"f1\": sklearn.metrics.f1_score(labels, predictions)}\n",
    "\n",
    "\n",
    "# Function to get the trainer\n",
    "def get_trainer(dataset, parameters):\n",
    "    training_args = transformers.TrainingArguments(\n",
    "        output_dir=FOLDER_TO_OUTPUT,\n",
    "        learning_rate=parameters[\"learning_rate\"],\n",
    "        per_device_train_batch_size=parameters[\"batch_size\"],\n",
    "        per_device_eval_batch_size=parameters[\"batch_size\"],\n",
    "        num_train_epochs=parameters[\"epochs\"],\n",
    "        weight_decay=parameters[\"weight_decay\"],\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_strategy=\"epoch\",\n",
    "        warmup_ratio=parameters[\"warmup_ratio\"],\n",
    "    )\n",
    "\n",
    "    return transformers.Trainer(\n",
    "        model_init=lambda: get_model(parameters, db_config_base, model_nm),\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01b8d0-003d-4405-a70c-99e1f6dab77c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "trainer = get_trainer(dataset, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b84720d-2394-4f7e-a82a-d53010ce5b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc388bd-46d2-4fbb-956e-f8910e71005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = prepare_dataset(ds_tweets, tokenizer, indices_train, indices_test, train=False)\n",
    "\n",
    "# Make predictions\n",
    "predictions = trainer.predict(test_dataset[\"test\"]).predictions\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# True labels\n",
    "true_labels = test_dataset[\"test\"][\"label\"]\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = sklearn.metrics.confusion_matrix(true_labels, predictions)\n",
    "print(conf_matrix)\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(true_labels, predictions)\n",
    "auc = sklearn.metrics.auc(fpr, tpr)\n",
    "\n",
    "# Plotting ROC Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"ROC curve (area = %0.2f)\" % auc)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aafd01-dc19-4a28-9567-d5d28d036943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the full dataset\n",
    "full_dataset = prepare_dataset(ds_tweets, tokenizer, indices_train, indices_test)\n",
    "\n",
    "# Initialize the trainer with the full dataset\n",
    "full_trainer = get_trainer(full_dataset, parameters)\n",
    "\n",
    "# Train on the full dataset\n",
    "full_trainer.train()\n",
    "\n",
    "# Evaluate and analyze the results as done in Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0882fbea-ab8f-441b-894a-bde8d9d23ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62137d-1e39-49ae-9098-6da403aa33ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52a0fd5d-f280-46c2-b2ff-9a193bfbc296",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d442d-4a57-43ab-a392-18d8517335e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    DebertaTokenizer,\n",
    "    DebertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# Load your dataset (replace this with your actual loading code)\n",
    "# For example, if your xarray dataset is named 'dataset':\n",
    "# df = dataset.to_dataframe()\n",
    "# Assuming a simple DataFrame with columns 'tweet' and 'label'\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"tweet\": [\n",
    "            \"It is raining today\",\n",
    "            \"What a sunny day\",\n",
    "            \"Raining again!\",\n",
    "            \"No rain today\",\n",
    "        ],\n",
    "        \"label\": [1, 0, 1, 0],  # 1 for 'Raining', 0 for 'Not Raining'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Text Preprocessing\n",
    "# Add any specific text preprocessing here if needed\n",
    "\n",
    "# Splitting the dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df[\"tweet\"], df[\"label\"], test_size=0.2)\n",
    "\n",
    "# Load DeBERTa tokenizer\n",
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "\n",
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)\n",
    "\n",
    "\n",
    "# Create a Dataset object\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = TweetDataset(val_encodings, val_labels.tolist())\n",
    "\n",
    "# Load DeBERTa model for sequence classification\n",
    "model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\", num_labels=2)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_result = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", evaluation_result)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./deberta_tweet_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3766877-3d2e-49b9-879a-735efd29fff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'evaluation_result' contains the results from the trainer.evaluate()\n",
    "# Example: evaluation_result = {'eval_loss': 0.123, 'eval_accuracy': 0.95, ...}\n",
    "\n",
    "# Extract metrics\n",
    "metrics = evaluation_result.keys()\n",
    "values = [evaluation_result[metric] for metric in metrics]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(metrics, values, color=\"blue\")\n",
    "plt.xlabel(\"Metrics\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.title(\"Model Evaluation Results\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881279fc-8b90-488e-9de0-fbc0bdd1d361",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f94235-5954-4dfa-8fd0-d8bcd1b5878a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    DebertaTokenizer,\n",
    "    DebertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df78bc-187c-4b09-9d29-58bf66c55fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "FOLDER_DATA = \"/p/project/deepacf/maelstrom/haque1/dataset/\"\n",
    "FOLDER_TWEET = FOLDER_DATA + \"tweets_2017_01_era5_normed_filtered.nc\"\n",
    "ds_tweets = xarray.load_dataset(FOLDER_TWEET)\n",
    "\n",
    "# Define labels based on a condition (e.g., a certain threshold)\n",
    "key_tp = \"tp_h\"  # Replace with your key\n",
    "ds_tweets[\"raining\"] = ([\"index\"], ds_tweets[key_tp].values > 1e-8)\n",
    "\n",
    "# Split the dataset\n",
    "indices_train, indices_test = train_test_split(np.arange(ds_tweets[\"index\"].shape[0]), test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the dataset to pandas DataFrame\n",
    "df = ds_tweets.to_dataframe().reset_index()\n",
    "df_train = df.loc[df[\"index\"].isin(indices_train)]\n",
    "df_test = df.loc[df[\"index\"].isin(indices_test)]\n",
    "\n",
    "train_texts = df_train[\"text_normalized\"].tolist()\n",
    "train_labels = df_train[\"raining\"].astype(int).tolist()\n",
    "val_texts = df_test[\"text_normalized\"].tolist()\n",
    "val_labels = df_test[\"raining\"].astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e296c63-a199-4e04-95b8-f7e3264e1c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b414cc4-2b22-4fa4-bc91-101134bc2654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "val_dataset = TweetDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52313d-c9dc-4929-b7b2-63bbe57f1c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "val_dataset = TweetDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7e584-105b-40e0-bee2-1386654777c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1adc26a-eb64-4253-90a9-db9d42f4909a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa4bf7-9de0-4c43-8bce-48ec5fc62fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a270d-2617-4390-835d-739a17835131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5979f07e-becd-4ae4-9e26-1ce157bcd874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_result = trainer.evaluate()\n",
    "print(\"Evaluation results:\", evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37e784-502d-4e4b-b6af-12bb22c08262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./deberta_tweet_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0901c-edab-4b23-be5a-4f6205f84ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../helpers/plotting\")\n",
    "import test_training_distribution\n",
    "import dataset_length_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b69a6f9-c56b-4c1d-a0ad-43a48ac987f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_training_distribution.plot_label_distribution(\n",
    "    df,\n",
    "    \"raining\",\n",
    "    title=\"Distribution of Tweets (Raining vs Not Raining)\",\n",
    "    x_label=\"Raining\",\n",
    "    y_label=\"Number of Tweets\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfee1eb-63a7-4f91-b672-d1e6b35ac190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_length_distribution.plot_numeric_distribution(\n",
    "    df,\n",
    "    \"text_normalized\",\n",
    "    bins=30,\n",
    "    title=\"Distribution of Tweet Lengths\",\n",
    "    x_label=\"Tweet Length\",\n",
    "    y_label=\"Frequency\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4a049-4c80-469a-a18a-7bfdec2dd8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"tweet_length\"] = df[\"text_normalized\"].apply(len)\n",
    "sns.histplot(df[\"tweet_length\"], bins=30)\n",
    "plt.title(\"Distribution of Tweet Lengths\")\n",
    "plt.xlabel(\"Tweet Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d909b-9087-4bd3-9abc-f1a5655897c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'test_ds' is your test dataset in the format expected by Hugging Face\n",
    "# test_ds = get_dataset(\n",
    "#     ds_tweets.sel(index=indices_test),\n",
    "#     tok_func,\n",
    "#     tokenizer,\n",
    "#     indices_train,\n",
    "#     indices_test,\n",
    "#     train=False\n",
    "# )\n",
    "test_ds = val_dataset\n",
    "# Make predictions\n",
    "preds_output = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a7fc5-852a-4ca6-bf68-c7e7fe624bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = torch.nn.functional.softmax(torch.Tensor(preds_output.predictions), dim=1).numpy()\n",
    "prediction_probability = preds[:, 1]  # Probability of 'Raining'\n",
    "predictions = preds.argmax(axis=-1)  # Predicted class (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d404a88-fba8-4e36-ad32-f94a9889b1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is a selection of your xarray dataset corresponding to the test set\n",
    "ds_test = ds_tweets.sel(index=indices_test)\n",
    "truth = ds_test.raining.values  # Actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a9d0a-7b54-4860-b89f-a5843ad13511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'predictions' and 'truth' are your model's predictions and the true labels\n",
    "report = classification_report(truth, predictions, target_names=[\"Not Raining\", \"Raining\"], output_dict=True)\n",
    "\n",
    "# Plotting the classification report\n",
    "sns.heatmap(pd.DataFrame(report).iloc[:-1, :].T, annot=True)\n",
    "plt.title(\"Classification Report\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6dd45-2b98-4204-9469-b856ddc8a803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import roc\n",
    "\n",
    "roc.plot_roc_curve(\n",
    "    truth,\n",
    "    prediction_probability,\n",
    "    title=\"My Custom ROC Title\",\n",
    "    color=\"red\",\n",
    "    linestyle=\"-.\",\n",
    "    linewidth=2.5,\n",
    "    legend_loc=\"upper left\",\n",
    "    figsize=(5, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba8643-64a4-4764-a06b-48f581b58807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(truth, prediction_probability)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2fd97-0ae8-47aa-8244-4da0f6c031c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import confusion_matrix\n",
    "\n",
    "confusion_matrix.plot_confusion_matrix(\n",
    "    truth,\n",
    "    predictions,\n",
    "    labels=[\"Class 0\", \"Class 1\"],\n",
    "    title=\"My Custom Confusion Matrix\",\n",
    "    cmap=\"Blues\",\n",
    "    figsize=(5, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03cd2fb-c07d-424e-ba34-b4dd2adaba86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(truth, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b1b8e-4c95-4389-99b2-bb7615c948dd",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58ed01-6d5b-4812-8de4-4527d52933ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import sys\n",
    "import transformers\n",
    "import datasets\n",
    "import functools\n",
    "import xarray as xr\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\"../helpers/plotting\")\n",
    "import test_training_distribution\n",
    "\n",
    "sys.path.append(\"../helpers\")\n",
    "from transformer_trainer import get_trainer\n",
    "\n",
    "ds_raw = xr.open_dataset(\"/p/project/deepacf/maelstrom/haque1/dataset/tweets_2017_01_era5_normed_filtered.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d873ab1-2636-49f2-a40d-34776b61ec91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# again define labels\n",
    "key_tp = \"tp_h\"\n",
    "ds_raw[\"raining\"] = ([\"index\"], ds_raw[key_tp].values > 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded38aed-ec08-47ed-abde-7fbe4deff2df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = ds_raw[\"raining\"]\n",
    "\n",
    "indices_train, indices_test = train_test_split(ds_raw.index, test_size=0.20, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69202e8-54e0-4775-8bb7-56f15696ea67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ds_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1f5dc-5f05-4d0f-823c-9d3ba8503cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_training_distribution.plot_label_distribution_split(\n",
    "#     ds_raw.index, [indices_train, indices_test],\n",
    "#     column='raining',\n",
    "#     titles=['Training Set Label Distribution',\n",
    "#             'Test Set Label Distribution'],\n",
    "#     x_label='Label',\n",
    "#     y_label='Frequency',\n",
    "#     figsize=(12, 6)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4d3c9-7ecc-4082-8298-f38b3e14a058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pretrained tokenizer and model configuration\n",
    "model_nm = \"/p/project/deepacf/maelstrom/haque1/deberta-v3-small\"  # Path to model\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_nm)\n",
    "db_config_base = transformers.AutoConfig.from_pretrained(model_nm, num_labels=2)\n",
    "\n",
    "\n",
    "# Define function to tokenize the field 'inputs' stored in x\n",
    "def tok_func(x, tokenizer):\n",
    "    return tokenizer(x[\"inputs\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "# Function to convert the dataset to a format used by Hugging Face\n",
    "def get_dataset(ds, tok_func, tokenizer, indices_train, indices_test, train=True):\n",
    "    df = ds[[\"text_normalized\", \"raining\"]].to_pandas()\n",
    "    df = df.rename(columns={\"text_normalized\": \"inputs\", \"raining\": \"labels\"})\n",
    "    datasets_ds = datasets.Dataset.from_pandas(df)\n",
    "    tok_function_partial = functools.partial(tok_func, tokenizer=tokenizer)\n",
    "    tok_ds = datasets_ds.map(tok_function_partial, batched=True)\n",
    "    if train:\n",
    "        return datasets.DatasetDict({\"train\": tok_ds.select(indices_train), \"test\": tok_ds.select(indices_test)})\n",
    "    else:\n",
    "        return tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe35745-03b8-4f68-b8dc-9aa2eb7a0358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = get_dataset(ds_raw, tok_func, tokenizer, indices_train, indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc90a2a-8d2f-4ea8-831b-830d44262e54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf62831-9db7-4e2e-bd11-2670583ddaae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER_TO_OUTPUT = \"./outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a3310-6a32-42fe-b859-da4dd2b7b93b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"learning_rate\": 8e-5,\n",
    "    \"batch_size\": 16,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"epochs\": 1,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"cls_dropout\": 0.3,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "}\n",
    "\n",
    "os.makedirs(FOLDER_TO_OUTPUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef66da-ec62-4094-a801-fc090434b4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = get_trainer(dataset, db_config_base, model_nm, FOLDER_TO_OUTPUT, parameters)\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc083bb-4de6-4014-ba54-27c010f4ac04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import sklearn.metrics\n",
    "import functools\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def get_model(params, db_config_base, model_nm):\n",
    "    db_config = db_config_base\n",
    "    if params is not None:\n",
    "        db_config.update({\"cls_dropout\": params[\"cls_dropout\"]})\n",
    "    db_config.update({\"num_labels\": 2})\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(model_nm, config=db_config)\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    classification_report = sklearn.metrics.classification_report(\n",
    "        labels, predictions, target_names=[\"not raining\", \"raining\"], output_dict=True\n",
    "    )\n",
    "    f1_not_raining = classification_report[\"not raining\"][\"f1-score\"]\n",
    "    f1_raining = classification_report[\"raining\"][\"f1-score\"]\n",
    "    return {\"f1_not_raining\": f1_not_raining, \"f1_raining\": f1_raining}\n",
    "\n",
    "\n",
    "def get_trainer(dataset, db_config_base, model_nm, FOLDER_TO_OUTPUT, parameters):\n",
    "    args = transformers.TrainingArguments(\n",
    "        FOLDER_TO_OUTPUT,\n",
    "        learning_rate=parameters[\"learning_rate\"],\n",
    "        warmup_ratio=parameters[\"warmup_ratio\"],\n",
    "        lr_scheduler_type=parameters[\"lr_scheduler_type\"],\n",
    "        disable_tqdm=False,\n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=parameters[\"batch_size\"],\n",
    "        per_device_eval_batch_size=parameters[\"batch_size\"],\n",
    "        num_train_epochs=parameters[\"epochs\"],\n",
    "        weight_decay=parameters[\"weight_decay\"],\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    get_model_partial = functools.partial(get_model, db_config_base=db_config_base, model_nm=model_nm)\n",
    "    return transformers.Trainer(\n",
    "        model_init=get_model_partial,\n",
    "        args=args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"learning_rate\": 8e-5,\n",
    "    \"batch_size\": 16,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"epochs\": 1,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"cls_dropout\": 0.3,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "}\n",
    "\n",
    "\n",
    "db_config_base = transformers.AutoConfig.from_pretrained(model_nm)\n",
    "\n",
    "\n",
    "os.makedirs(FOLDER_TO_OUTPUT, exist_ok=True)\n",
    "\n",
    "trainer = get_trainer(dataset, db_config_base, model_nm, FOLDER_TO_OUTPUT, parameters)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e8dd9-5c30-4a1e-9dea-542ed74c6b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is the test dataset in the format expected by Hugging Face\n",
    "test_ds = get_dataset(\n",
    "    ds_raw.sel(index=indices_test),\n",
    "    tok_func,\n",
    "    tokenizer,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    "    train=False,  # not training anymore\n",
    ")\n",
    "# this is a selection of our xarray dataset that corresponds to the tweets that are part of the test set\n",
    "ds_test = ds_raw.sel(index=indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d71e74-b9f6-436e-8bc0-e414d1f15d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "import plotting\n",
    "\n",
    "preds = torch.nn.functional.softmax(torch.Tensor(trainer.predict(test_ds).predictions)).numpy()\n",
    "prediction_probability = preds[:, 1]\n",
    "predictions = preds.argmax(axis=-1)\n",
    "truth = ds_test.raining.values\n",
    "plotting.analysis.classification_report(labels=truth, predictions=predictions)\n",
    "plotting.analysis.plot_roc(truth=truth, prediction_probability=prediction_probability)\n",
    "plotting.plotting.analysis.check_prediction(truth=truth, prediction=predictions);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0de51-2634-4947-a395-14db2b923d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_saved_trained_model(ds, folder_to_model, db_config_base, model_nm, parameters):\n",
    "    # load the pretrained tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(folder_to_model)\n",
    "    db_config_base = transformers.AutoConfig.from_pretrained(folder_to_model, num_labels=2)\n",
    "    dataset = get_dataset(ds, tok_func, tokenizer, indices_train, indices_test)\n",
    "    trainer = get_trainer(dataset, db_config_base, folder_to_model, folder_to_model, parameters)\n",
    "    return trainer\n",
    "\n",
    "\n",
    "trainer_evaluate = load_saved_trained_model(\n",
    "    ds_raw,\n",
    "    FOLDER_TO_OUTPUT + FOLDER_TO_OUTPUT,\n",
    "    db_config_base,\n",
    "    model_nm,\n",
    "    parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e7673e-3cea-40d4-bf72-8d0f82918e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap2",
   "language": "python",
   "name": "ap2_hf-llm-bnb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
