{
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Introduction to LLM model Falcon\n",
"\n",
"Currently, our dataset contains Tweets that should provide precipitation related information as Tweets have been filtered based on keywords/emojies that are related to this field. However, clearly many Tweets contained do not provide sufficient information even for a human to decide if it was \"raining\"/\"not raining\" at the location of the Tweeter. \n",
"\n",
"To build a more robust dataset for training the rain classifier, we would like to train an additional classifier that decides if the Tweet contains \"relevant\" information that would allow the rain classifier to make an educated guess. To train this \"relevance\" classifier, we would like to\n",
"leverage an LLM that labels our data. Here, we build some prompts that entice the LLM to judge if the Tweet contains information related to the presence of rain/sentence and test how the model reacts.\n",
"\n",
"In addition, code exists that reduces the output files generated by the responses of the LLM.\n",
"\n",
"Note, this notebook requires kernel `ap2_HF-LLM-BnB` instead of `ap2`!"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"!jupyter kernelspec list"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"# allows update of external libraries without need to reload package\n",
"%load_ext autoreload\n",
"%autoreload 2"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"import numpy as np\n",
"import transformers\n",
"import torch\n",
"import xarray\n",
"import re\n",
"import os\n",
"import glob\n",
"import sys\n",
"\n",
"\n",
"import a2.utils\n",
"\n",
"import a2.training.training_hugging\n",
"import a2.training.evaluate_hugging\n",
"import a2.training.dataset_hugging\n",
"import a2.plotting.analysis\n",
"import a2.plotting.histograms\n",
"import a2.dataset"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"# a2.training.utils_training.gpu_available()\n",
"torch.cuda.empty_cache()"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"FOLDER_MODEL = \"/p/project/deepacf/maelstrom/ehlert1/models/falcon-40b\"\n",
"!ls $FOLDER_MODEL"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"FOLDER_DATA = \"/p/project/training2330/a2/data/bootcamp2023/\"\n",
"FOLDER_TWEETS = FOLDER_DATA + \"tweets/\"\n",
"FILE_TWEETS = FOLDER_TWEETS + \"tweets_2017_01_era5_normed_filtered.nc\""
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"!ls -Rtlh $FOLDER_DATA"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"ds = a2.dataset.load_dataset.load_tweets_dataset(FILE_TWEETS)\n",
"ds"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"ds[\"relevance_hand\"] = ([\"index\"], np.ones_like(ds.index.values))\n",
"ds[[\"text\", \"raining\", \"raining_station\", \"relevance_hand\"]].to_pandas().to_csv(\n",
"    \"tweets_2017_01_era5_normed_filtered.csv\"\n",
")"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"ds[\"raining\"] = ([\"index\"], np.array(ds.tp_mm_station.values > 6e-3, dtype=int))"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"!cat $FOLDER_MODEL/tokenizer_config.json"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"bnb_config = transformers.BitsAndBytesConfig(\n",
"    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
")\n",
"\n",
"model = transformers.AutoModelForCausalLM.from_pretrained(\n",
"    FOLDER_MODEL, device_map=\"auto\", trust_remote_code=False, quantization_config=bnb_config\n",
")\n",
"\n",
"tokenizer = transformers.AutoTokenizer.from_pretrained(FOLDER_MODEL)\n",
"tokenizer.pad_token = tokenizer.eos_token"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"transformers.__version__"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Analyze misclassified Tweets\n",
"\n",
"Let's make predictions with our previously trained model and build a dataset of Tweets that were previously misclassified. We expect a large portion of them to not have sufficient information for the model to accurately estimate the presence of rain. "
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"folder_model = \"/p/scratch/deepacf/unicore-jobs/1ec1cdb1-1245-4605-addb-2626f3ed2aab/finetune_rain/checkpoint-5500\"  # change to your models\n",
"\n",
"truth, predictions, prediction_probabilities = a2.training.evaluate_hugging.make_predictions_loaded_model(\n",
"    ds, indices_validate=ds.index.values, folder_model=folder_model, key_inputs=\"text_normalized\"\n",
")\n",
"\n",
"miss = truth + predictions\n",
"ds_miss = ds.sel(index=np.arange(len(ds.index.values))[miss == 1])\n",
"ds_miss"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### Let's throw out Tweets mentioning snow as it seems to confuse the model as well"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"ds_no_snow = ds_miss.where(~ds_miss.text_normalized.str.contains(\"snow\", flags=re.IGNORECASE), drop=True)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"def tokenize_prompt(prompt):\n",
"    return tokenizer.encode(prompt, return_tensors=\"pt\").cuda()"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Simple prompt"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"prompt = r\"\"\"\n",
"Does the following sentence provide information on presence of rain? Explain your reasoning.\n",
"\n",
"Sentence: It is raining in London.\n",
"\"\"\"\n",
"input_ids = tokenize_prompt(prompt)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"sample_outputs = model.generate(\n",
"    input_ids,\n",
"    temperature=0.7,\n",
"    # do_sample=True,\n",
"    max_length=100,\n",
"    # top_k=50,\n",
"    # top_p=0.95,\n",
"    # num_return_sequences=3\n",
")\n",
"\n",
"for i, sample_output in enumerate(sample_outputs):\n",
"    prediction = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
"    print(f\"{prompt=}\")\n",
"    print(f\"---------\")\n",
"    print(f\"prediction\\n{prediction}\")"
]
},
{
"cell_type": "markdown",
"metadata": {
"tags": []
},
"source": [
"## More complex instructions"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"# prompt = r\"\"\"\n",
"# Read below Tweets and tell me if they say that it is raining or sunny.\n",
"\n",
"# Format your answer in a human readable way,\n",
"\n",
"prompt = r\"\"\"\n",
"Read below Tweets and tell me if they say that it is raining or sunny.\n",
"Format your answer in a human readable way,\n",
"\n",
"Tweets:\n",
"Tweet 1: \"The sound of rain tapping on the window\" \n",
"Tweet 2: \"Boris likes drinking water\". \n",
"\"\"\"\n",
"\n",
"example_output = \"\"\"\n",
"Return the results in a json file like: [ \n",
"{ \"tweet\": 1, \"content\": \"The sound of rain tapping on the window\", \"explanation\": \"The sound of rain heard implies that is raining.\", \"score\": 0.9 },  \n",
"{ \"tweet\": 2, \"content\": \"Boris likes drinking water\", \"explanation\": \"The Tweet does not mention any information related to presence of rain or sun.\", \"score\": 0.1},\n",
"{ \"tweet\": 3, \"content\": ... \n",
"] \n",
"\n",
"Result: [ { \"tweet\": 1, \"content\":\"\"\"\n",
"\n",
"tweets = ds_no_snow[\"text_normalized\"][np.random.choice(np.arange(len(ds_no_snow[\"text_normalized\"].values)), 5)].values\n",
"# ds_no_snow[\"text_normalized\"].values[120:124]\n",
"\n",
"\n",
"def string_tweets(tweets):\n",
"    string = \"\"\n",
"    for i_tweet, t in enumerate(tweets):\n",
"        string += f'Tweet {i_tweet + 3}: \"{t}\"\\n'\n",
"    return string"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"%%time\n",
"\n",
"\n",
"def string_tweets(tweets):\n",
"    string = \"\"\n",
"    for i_tweet, t in enumerate(tweets):\n",
"        string += f'Tweet {i_tweet + 3}: \"{t}\"\\n'\n",
"    return string\n",
"\n",
"\n",
"def generate_prediction(args, tokenizer, model, prompt, tweets, example_output):\n",
"    full_prompt = prompt + string_tweets(tweets) + example_output\n",
"    input_ids = tokenize_prompt(full_prompt)\n",
"\n",
"    sample_outputs = model.generate(\n",
"        input_ids,\n",
"        temperature=0.9,\n",
"        # do_sample=True,\n",
"        max_length=650,\n",
"        top_k=50,\n",
"        # top_p=0.95,\n",
"        # num_return_sequences=3\n",
"    )\n",
"\n",
"    for i, sample_output in enumerate(sample_outputs):\n",
"        prediction = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
"        print(f\"{prompt=}\")\n",
"        print(f\"---------\")\n",
"        print(f\"prediction\\n{prediction}\")\n",
"        return prediction"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"ds_no_snow = ds.where(~ds.text_normalized.str.contains(\"snow\", flags=re.IGNORECASE), drop=True)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"prompt = r\"\"\"\n",
"Read below Tweets and tell me if they say that it is raining or sunny. It should be rainy or sunny now.\n",
"Format your answer in a human readable way,\n",
"\n",
"Tweets:\n",
"Tweet 1: \"The sound of rain tapping on the window\" \n",
"Tweet 2: \"Boris likes drinking water\". \n",
"\"\"\"\n",
"\n",
"example_output = \"\"\"\n",
"Return the results in a json file like: [ \n",
"{ \"tweet\": 1, \"content\": \"The sound of rain tapping on the window\", \"explanation\": \"The sound of rain heard implies that is raining.\", \"score\": 0.9 },  \n",
"{ \"tweet\": 2, \"content\": \"Boris likes drinking water\", \"explanation\": \"The Tweet does not mention any information related to presence of rain or sun.\", \"score\": 0.1},\n",
"{ \"tweet\": 3, \"content\": ... \n",
"] \n",
"\n",
"Result: [ { \"tweet\": 1, \"content\":\"\"\"\n",
"n_samples = 10\n",
"N_start = 5000\n",
"args = None\n",
"tweets = ds_no_snow[\"text_normalized\"].values[slice(N_start, N_start + n_samples)]\n",
"\n",
"for tweet_sample in np.array_split(tweets, len(tweets) // 5):\n",
"    prediction = generate_prediction(args, tokenizer, model, prompt, tweet_sample, example_output)\n",
"    with open(\"dump_relevance.csv\", \"a\") as fd:\n",
"        fd.write(prediction)"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Further ideas for prompts ...\n",
"Have a look at the following prompts and eperiment with your own.\n",
"You may als vary the `temperature`, which increases the creativity of the model if its replies are too \"conservative\".\n",
"Additionally, we have `top_k`, `top_p` and `num_return_sequences` that may help your prompts \"succeed\"."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"prompt = r'Assign a probability that it is raining to the following tweets. The content of the tweets should hint something about the weather being rainy and not good in general. Tweet 1: \"Well that last rumble of thunder made the house shake, I wasn\\'t scared for a couple of seconds\" Tweet 2: \"#viewfromthe office what a great morning @Grantham and District\". Return the results in a json file like: [ { \"tweet\": 1, \"content\": \"Well that last rumble of thunder made the house shake, I wasn\\'t scared for a couple of seconds\", \"explanation\": \"short explanation of the rain probability based on content of this tweet\", \"rain_probability\": x.x }, ... ] Result: [ { \"tweet\": 1, \"content\":'\n",
"prompt = r\"\"\"Assign a probability that it is raining to the following tweets. The content of the tweets should hint something about the weather being rainy and not good in general. \n",
"Tweet 1: \"The sound of rain tapping on the window\" \n",
"Tweet 2: \"Boris seems desperate for the rain to finish\". \n",
"Return the results in a json file like: [ { \"tweet\": 1, \"content\": \"The sound of rain tapping on the window\", \"explanation\": \"The sound of rain heard implies that is raining.\", \"rain_probability\": 0.9 },  { \"tweet\": 2, \"content\": \"Boris seems ... ] \n",
"Result: [ { \"tweet\": 1, \"content\":\"\"\"\n",
"prompt = r\"\"\"\n",
"Assign a probability that it is raining or not rainy to the following tweets, where 1 corresponds to rainy conditions, and 0 to not rainy/sunny conditions. \n",
"In addition to that, assign a score that quantifies the certainty of your assessment based on the content of the tweet, regardless of whether it is rainy or not. \n",
"Thus, for example, if a tweet doesn't mention anything weather-related, that score should be 0. \n",
"If the tweet mentions a sunny day, the probability for rain should be close to 0, and the certainty should be close to 1 as the tweet includes explicit information about the weather. \n",
"The content of the tweets should hint something about the weather being rainy and not good in general. \n",
"List all tweets even if the scores and probabilities are 0. In addition, provide a brief explanation of your assessments for each tweet.\n",
"\n",
"Format your answer in a human readable way,\n",
"\n",
"Tweets:\n",
"Tweet 1: \"The sound of rain tapping on the window\" \n",
"Tweet 2: \"Boris seems desperate for the rain to finish\". \n",
"\"\"\""
]
},
{
"cell_type": "markdown",
"metadata": {
"tags": []
},
"source": [
"## Reduce outputs"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"f = open(\"/p/project/training2330/a2/data/bootcamp2023/relevance/dump_relevance_5000.csv\", \"r\")\n",
"file = f.read()"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"def extract_from_file(file):\n",
"    full_tweet_replies = re.findall('(\\{ \"tweet\": [3-9].+\\})', file)\n",
"    CONTENTS = []\n",
"    EXPLANATIONS = []\n",
"    SCORES = []\n",
"    for reply in full_tweet_replies:\n",
"        content = re.findall('\"content\": \"(.+)\", \"expl', reply)\n",
"        explanation = re.findall('\"explanation\": \"(.+)\", \"score', reply)\n",
"        score = re.findall('\"score\": ([0-9]+\\.[0-9]+)[\\s]{0,1}\\}', reply)\n",
"        CONTENTS.append(content)\n",
"        EXPLANATIONS.append(explanation)\n",
"        SCORES.append(score)\n",
"\n",
"    return np.array(CONTENTS).reshape(-1), np.array(EXPLANATIONS).reshape(-1), np.array(SCORES, float).reshape(-1)\n",
"\n",
"\n",
"def add_file_content(SCORES, EXPLANATION, CONTENTS):\n",
"    for c, e, s in zip(contents, explanations, scores):\n",
"        index = np.where(ds.text_normalized.values == c)[0]\n",
"        if len(index) != 1:\n",
"            print(f\"{index=}, couldn't match {c}.\")\n",
"            print(f\"{s=}\")\n",
"            continue\n",
"        index = index[0]\n",
"        CONTENTS[index] = c\n",
"        EXPLANATION[index] = e\n",
"        SCORES[index] = s\n",
"    return SCORES, EXPLANATION, CONTENTS"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"!ls /p/scratch/deepacf/unicore-jobs/*/dump_relevance.csv"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"complesDay3/06NewModels.ipynbRELEVANCE_FILES = glob.glob(\"/p/scratch/deepacf/unicore-jobs/*/dump_relevance.csv\")\n",
"print(f\"{RELEVANCE_FILES=}\")\n",
"SCORES = np.ones_like(ds.index.values, dtype=float) * -1\n",
"EXPLANATION = np.empty_like(ds.index.values, dtype=object)\n",
"CONTENTS = np.empty_like(ds.index.values, dtype=object)\n",
"Day3/06NewModels.ipynb\n",
"for filename in RELEVANCE_FILES:\n",
"    f = open(filename, \"r\")\n",
"    file = f.read()\n",
"    contents, explanations, scores = extract_from_file(file)\n",
"    SCORES, EXPLANATION, CONTENTS = add_file_content(SCORES, EXPLANATION, CONTENTS)\n",
"ds[\"relevance_score\"] = ([\"index\"], SCORES)\n",
"ds[\"explanation\"] = ([\"index\"], EXPLANATION)\n",
"ds[\"contents\"] = ([\"index\"], CONTENTS)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"plt.hist(scores, bins=100);"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"ds[\"relevance_score\"].plot.hist()"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": []
},
"outputs": [],
"source": [
"ds.to_netcdf(\n",
"    \"/p/project/training2330/a2/data/bootcamp2023/relevance/tweets_2017_01_era5_normed_filtered_relevance_score.nc\"\n",
")"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"ds.where(ds.relevance_score\"] "
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": []
}
],
"metadata": {
"kernelspec": {
"display_name": "ap2_HF-LLM-BnB",
"language": "python",
"name": "hffinetuningbnb"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.10.12"
}
},
"nbformat": 4,
"nbformat_minor": 4
}