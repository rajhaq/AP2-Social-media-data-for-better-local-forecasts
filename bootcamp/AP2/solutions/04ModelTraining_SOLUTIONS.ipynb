{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb52a76-7b06-4946-9e97-3d9e0b0fcefb",
   "metadata": {},
   "source": [
    "## Preamble: Execute this if checking any answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccbbb3f-3286-4b16-973d-062e72550bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows update of external libraries without need to reload package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41eb48-32d6-4054-a8c0-fa0219397cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f8f60-00ab-4372-9bb0-747e24b69816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import xarray\n",
    "\n",
    "# location of scripts folder for bootcamp\n",
    "sys.path.append(\"/p/home/jusers/ehlert1/juwels/notebooks/bootcamp_testing/scripts\")\n",
    "import normalize_text_bootcamp\n",
    "import utils_bootcamp\n",
    "import plotting\n",
    "import dataset_bootcamp\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc46d3-00ab-403e-a8e4-acbe88e03aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# may take some time ...\n",
    "import sys\n",
    "import pathlib\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import functools\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray\n",
    "\n",
    "# Pytorch modules\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "# scikit-learn modules\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "\n",
    "# \"Hugging Face\" modules\n",
    "import datasets\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049751a-5535-4bd0-898b-58c9dc804c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_TO_TWEETS = \"/p/project/training2223/a2/data/tweets/tweets_2017_era5_normed_filtered.nc\"\n",
    "# FOLDER_TO_TWEETS = \"../../data/tweets/tweets_2017_normalized.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0704c5-6001-414d-8a88-752538666c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /p/project/training2223/a2/data/tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6818e1a1-46b9-4da9-b50e-f52fb12e4054",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc0308-1278-4e3f-810b-64804408aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets_dataset():\n",
    "    ds = xarray.load_dataset(FOLDER_TO_TWEETS)\n",
    "    ds = dataset_bootcamp.reset_index_coordinate(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d8468-0e63-4c8e-b945-b9a52d9a4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tweets = load_tweets_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f753839-c64b-451f-9c64-33a76eeca65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# again define labels\n",
    "key_tp = \"tp_h\"\n",
    "ds_tweets[\"raining\"] = ([\"index\"], ds_tweets[key_tp].values > 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c014db92-a173-4ead-8cff-d22d1fa7f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train, indices_test = sklearn.model_selection.train_test_split(\n",
    "    np.arange(ds_tweets[\"index\"].shape[0]),\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    test_size=0.2,\n",
    "    stratify=ds_tweets.raining.values,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8cf72-fbfb-44cf-86a5-30b70a06b1f0",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa45754-0dcc-472a-8325-e9ad17a90e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained tokenizer\n",
    "model_nm = (\n",
    "    \"/p/project/training2223/a2/models/deberta-v3-small/\"  # model repo downloaded from Hugging Face to the cluster\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_nm)\n",
    "db_config_base = transformers.AutoConfig.from_pretrained(model_nm, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b47b79-01b5-4a45-b113-b3753da0f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to convert the dataset to a format that is used by Hugging Face\n",
    "\n",
    "\n",
    "def tok_func(x, tokenizer):\n",
    "    \"\"\"\n",
    "    tokenizes the field 'inputs' stored in x including padding\n",
    "    \"\"\"\n",
    "    return tokenizer(x[\"inputs\"], padding=True)\n",
    "\n",
    "\n",
    "def get_dataset(ds, tok_func, tokenizer, indices_train, indices_test, train=True):\n",
    "    \"\"\"\n",
    "    converts dataset to 'dataset' format required by Hugging Face\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    ds: dataset\n",
    "    tok_func: functiond use for tokenization\n",
    "    indices_train: indices corresponding to the training set\n",
    "    indices_test: indices corresponding to the training set\n",
    "    train: if used for training\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    header of file\n",
    "    \"\"\"\n",
    "    # converting dataset to pandas as Hugging Face datasets has inbuilt function that converts pandas dataframe to a Hugging Face dataset\n",
    "    df = ds[[\"text_normalized\", \"raining\"]].to_pandas()\n",
    "    df = df.rename(columns={\"text_normalized\": \"inputs\"})\n",
    "    df = df.rename(columns={\"raining\": \"label\"})\n",
    "    datasets_ds = datasets.Dataset.from_pandas(df)\n",
    "    tok_function_partial = functools.partial(tok_func, tokenizer=tokenizer)\n",
    "    tok_ds = datasets_ds.map(tok_function_partial, batched=True)\n",
    "    if train:\n",
    "        return datasets.DatasetDict({\"train\": tok_ds.select(indices_train), \"test\": tok_ds.select(indices_test)})\n",
    "    else:\n",
    "        return tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc65c9b-3b1a-4167-9762-64b349a8be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Hugging Face 'dataset'\n",
    "dataset = get_dataset(ds_tweets, tok_func, tokenizer, indices_train, indices_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444c3a8-8bac-4cbb-8f98-88ff13da4ef4",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c515d6-e675-46c8-aa6e-9ecd0981846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "parameters[\"learning_rate\"] = 8e-5\n",
    "parameters[\"batch_size\"] = 16\n",
    "parameters[\"weight_decay\"] = 0.01\n",
    "parameters[\"epochs\"] = 1\n",
    "parameters[\"warmup_ratio\"] = 0.1\n",
    "parameters[\"cls_dropout\"] = 0.3\n",
    "parameters[\"lr_scheduler_type\"] = \"cosine\"\n",
    "\n",
    "FOLDER_TO_OUTPUT = \"/p/project/training2223/a2/models/output_debertav3_tweets_2017/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a645c-eb8a-4601-b1b2-d442fe7dfb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(params, db_config_base, model_nm):\n",
    "    \"\"\"\n",
    "    function to retrieve model, format follows Hugging Face convention (parameter -> 'params')\n",
    "    \"\"\"\n",
    "    db_config = db_config_base\n",
    "    if params is not None:\n",
    "        db_config.update({\"cls_dropout\": params[\"cls_dropout\"]})\n",
    "    db_config.update({\"num_labels\": 2})\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(model_nm, config=db_config)\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    compute f1 metrics of both labels, format follows Hugging Face convention\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    eval_pred: evaluation/test set probalities for classification task\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary returning labeled f1 score of \"not raining\" and \"raining\"\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    classification_report = sklearn.metrics.classification_report(\n",
    "        labels, predictions, target_names=[\"not raining\", \"raining\"], output_dict=True\n",
    "    )\n",
    "    f1_not_raining = classification_report[\"not raining\"][\"f1-score\"]\n",
    "    f1_raining = classification_report[\"raining\"][\"f1-score\"]\n",
    "    return {\"f1_not_raining\": f1_not_raining, \"f1_raining\": f1_raining}\n",
    "\n",
    "\n",
    "def get_trainer(dataset, db_config_base, model_nm, FOLDER_TO_OUTPUT, parameters):\n",
    "    \"\"\"\n",
    "    initializes `transformers.Trainer`, which is used to train models with Hugging Face\n",
    "\n",
    "    Hyper parameters are here assigned to model.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataset: dataset in format required by Hugging Face\n",
    "    db_config_base: default model configurations\n",
    "    model_nm: model folder\n",
    "    FOLDER_TO_OUTPUT: folder where trained model, tokenizer,... will be saved\n",
    "    parameters: dictionary of hyper-parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    trainer with assigned parameters used for training\n",
    "    \"\"\"\n",
    "    args = transformers.TrainingArguments(\n",
    "        FOLDER_TO_OUTPUT,\n",
    "        learning_rate=parameters[\"learning_rate\"],\n",
    "        warmup_ratio=parameters[\"warmup_ratio\"],\n",
    "        lr_scheduler_type=parameters[\"lr_scheduler_type\"],\n",
    "        disable_tqdm=False,\n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=parameters[\"batch_size\"],\n",
    "        per_device_eval_batch_size=parameters[\"batch_size\"],\n",
    "        num_train_epochs=parameters[\"epochs\"],\n",
    "        weight_decay=parameters[\"weight_decay\"],\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    # convert `get_model` to partial function to pass it as an argument in `transformers.Trainer`\n",
    "    # see https://www.geeksforgeeks.org/partial-functions-python/ for quick tutorial\n",
    "    get_model_partial = functools.partial(get_model, db_config_base=db_config_base, model_nm=model_nm)\n",
    "    return transformers.Trainer(\n",
    "        model_init=get_model_partial,\n",
    "        args=args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907213b-ce0b-469f-9185-88029d99c1a9",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c532d271-0b7d-464e-92e5-9b7d398615e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = get_trainer(dataset, db_config_base, model_nm, FOLDER_TO_OUTPUT, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d74cd-d7fc-4663-930e-3764f6a34db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46d752-97a4-44f8-a79c-cca7a3c17d67",
   "metadata": {},
   "source": [
    "## Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba45b58-518e-4f9a-b311-80e50501c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if loading required of saved model\n",
    "def load_saved_trained_model(ds, FOLDER_TO_OUTPUT, db_config_base, model_nm, parameters):\n",
    "    # load the pretrained tokenizer\n",
    "    model_nm = FOLDER_TO_OUTPUT\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_nm)\n",
    "    db_config_base = transformers.AutoConfig.from_pretrained(model_nm, num_labels=2)\n",
    "    dataset = get_dataset(ds, tok_func, tokenizer, indices_train, indices_test)\n",
    "    trainer = get_trainer(dataset, db_config_base, model_nm, FOLDER_TO_OUTPUT, parameters)\n",
    "    return trainer\n",
    "\n",
    "\n",
    "trainer_evaluate = load_saved_trained_model(\n",
    "    ds_tweets,\n",
    "    FOLDER_TO_OUTPUT + \"checkpoint-4605/\",\n",
    "    db_config_base,\n",
    "    model_nm,\n",
    "    parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a372b85-1fbd-4c20-9ba3-ec8ef399e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain test dataset in Huggin Face format\n",
    "test_ds = get_dataset(\n",
    "    ds_tweets.sel(index=indices_test),\n",
    "    tok_func,\n",
    "    tokenizer,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    "    train=False,  # not training anymore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27127120-a518-41a5-8e25-08afd71c10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds = torch.nn.functional.softmax(torch.Tensor(trainer.predict(test_ds).predictions)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857cfe57-59bf-4286-98f8-4c10d1412a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_tweets.sel(index=indices_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ec2d3-e7e7-47df-8084-a19bc8a78101",
   "metadata": {},
   "source": [
    "## 7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbce28f-9419-4b5a-9b38-67920f2d0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "truth = ds_test.raining.values\n",
    "prediction = preds.argmax(-1)\n",
    "report = plotting.analysis.check_prediction(truth, prediction)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7195db-ca41-4508-b2be-fa5fafaa707a",
   "metadata": {},
   "source": [
    "## Task 7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d53d1-a4fd-40a6-a78c-8430f4c46541",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = ds_test.raining.values\n",
    "prediction_probability = preds[:, 1]\n",
    "\n",
    "plotting.analysis.plot_roc(truth, prediction_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c09bf-52fb-4a45-90e3-ebb24f31a0a3",
   "metadata": {},
   "source": [
    "## 7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ee2ec-0dff-499b-9033-a6c0bc66ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = ds_test.raining.values\n",
    "prediction_probability = preds[:, 1]\n",
    "\n",
    "plotting.analysis.plot_predictions_confidence(\n",
    "    truth,\n",
    "    prediction_probability,\n",
    "    bins=10,\n",
    "    x_label=\"raining\",\n",
    "    y_label=\"preds_raining\",\n",
    "    filename=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a72ea2-abf0-4e65-8cfd-190ecdbc5419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap2",
   "language": "python",
   "name": "ap2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
