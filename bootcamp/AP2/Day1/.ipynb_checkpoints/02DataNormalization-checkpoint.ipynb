{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66746948-6567-4953-a1be-7129e66c59a8",
   "metadata": {},
   "source": [
    "# Application 2: Normalization of Tweets\n",
    "Normalizing data, i.e. bringing the text into a format that is preferable to the model, is a crucial part when doing basically any form of NLP.\n",
    "We would like to remove text, which the model has not been trained for or which is just irrevelant to our task. For this, we will need to do a lot of string manipulation. \n",
    "\n",
    "Regular expressions (sequence of characters that specify a search pattern in text) are key for this, which we will revise first. Without prior experience in regex the tasks in Exercise 0 and Exercise 1 may take quite some time, which would take time away from our machine learning applications.  \n",
    "Therefore, we suggest you to maybe quickly read through the Exercises and Tasks to at least get some idea of the concepts but **skip the Tasks in Exercise 0 and Exercise 1** and rather focus on Tasks for Exercise 2 for now. You are very welcome to look at that them by yourself later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327459c-80f5-4b21-b0da-f5e109f7288f",
   "metadata": {},
   "source": [
    "## Exercise 0: Regular expressions\n",
    "[Regular expressions](https://en.wikipedia.org/wiki/Regular_expression) are a very powerful tool to find and replace patterns found in text based data like our Tweets. While very complex expressions can be formed, we will try to provide a beginner-friendly introduction here. There are some basic operators which, will be provided in the following. However, you can of course use google to solve the tasks, just also try to understand why the suggested operation works. Note, usually expressions are more or less independent of the programming language used but if you want to google a certain operation, adding \"python\" to the search query will usually give you exactly what you need with no additional minor \"translations\" necessary.\n",
    "\n",
    "Python uses the package `re` (import via `import re`), which is a standard package. Usually, we would like to replace a string with a different one or delete certain strings, which is a replacement with an empty string. For this we use ` re.sub(pattern, repl, string, count=0, flags=0)`:\n",
    "* pattern: Is the regular expression determining what should be replaced\n",
    "* repl: Is the string/regular expression you want to replace `pattern` with (use `pattern=''` to delete).\n",
    "* string: Is the text you want to apply your replacement to, so usually a single Tweet in our case.\n",
    "* flags: Optional flags that can be used to alter the behavior of the function. For example, `re.IGNORECASE` can be useful if you want to do a case insensitive replacement. A list of the flags is given [here](https://docs.python.org/3/library/re.html#contents-of-module-re).\n",
    "\n",
    "\n",
    "Special characters are crucial to match broader search terms. Find a list at the begging of [their documentation](https://docs.python.org/3/library/re.html#contents-of-module-re). Don't bother to remember them initially, usually it makes sense to look for the correct one depending on your use case. Let's now take a look at some small examples taken from the project:\n",
    "* `re.sub(r\"We're\", \"We are\", text)`: Here, we are replacing the colloquial form \"We're\" with the (in written form) more common \"We are\" to ease training. This is a regular expression that just directly matches any occurence of exactly these charaters.\n",
    "* `re.sub(\"_+\", \" \", text)`: The plus symbol \"+\" is a special character to match 1 or more occurences of the preceding expression. Here, it will match any occurences of the underscore symbol \"_\" and replace it with a single space. Sometimes people use underscores to highlight parts of their text. However, in our training set for our model huge databases of more text formats are used like Wikipedia, where using underscores in this form is quite unlikely...\n",
    "* `re.sub(\"\\s+\", \" \", text)`: The special character \"\\s\" is used to denominate any whitespace characters (single spaces, tabs, new lines,..). Tweets can have multiple white space characters in a row, while we just want a single space between words. Therefore, this expression is replacing one or more consequetive whitespace characters with a single space.\n",
    "* `re.sub(\"@[a-z0-9]\", \"\", text)`: Using square brackets, e.g. `[abc]`, you can invoke a so called \"character class\". This means \"any character from a,b or c\" (a character class may use ranges, e.g. [a-d] = [abcd]) is matched. This means that our example matches any occurence of the at sign either followed by any number or any lower case letter and removes them.\n",
    "* `re.sub(\"@([a-z0-9])\", \"\\\\1\", text)`: This example is very reminiscent of the previous one. However, we introduce parentheses that are used for \"capture groups\". These allow you to reference your matched strings and use them later on. In this example, we again match at sign and a single following lower case character or number. However we assign the following single number or single lower case character to a capture group and reference our first capture group via `\\\\1` (when using raw string only use single backslash: `r'\\1'`) to effectively remove the at sign but leave the number/character untouched.\n",
    "* `re.sub(\"@?… https:\\S*$\", \"\", text)`: Let's end on a more complex expression. Some Tweets are shortened, where removed characters are replaced with the `…` character (Single character *not* three periods in a row) and contain links at the end.\n",
    "\n",
    "    (It's probably related to users sending Instagram messages when their Instagram and Twitter accounts are linked. If the message is too long, the Tweet will be cut short and removed words are replaced by a single `…` The same message will be posted on Twitter with the link to the instagram message appended to the end of the Tweet.)\n",
    "\n",
    "    This suffix is sometimes initialized with an at sign \"@\". To match a single or no occurence of a character, we append the character with the special character `?` (Consequently, need to *escape* the character if want to match a question mark, i.e. `\\?`). Then always the characters `… https:` follow. Finally, we would like to match any number of non-white space characters until the end of the sentence. We use the special character `\\S` to match a non-white space character. The special character `*` is appended to match zero or more appearances of the preceding character. The special character `$` then matches the end of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc869e31-640f-4593-8ffc-e7de4c7f3b0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tasks (skip!):\n",
    "Now, it is time for you to get started with this very useful toolbox used by most programmers independent of their field of work. Some of the tasks may be a bit overwhelming if you had no prior experience with regular expression. While you are always encouraged to cooperate on tasks. I would like to stress that you should feel free to reach out to your colleagues or any instructor for hints and tipps if you get stuck!\n",
    "\n",
    "Write a function using `re.sub` that ...\n",
    "* ... takes a string as an input and returns a sentence where all occurences of 'Hello!' are replaced with 'Bye!' \n",
    "\n",
    "    Example: 'Hello! Have a great day!' -> 'Bye! Have a great day!').\n",
    "* ... removes a hashtag which is proceded by any letter of the English alphabet independent of capitalization, e.g. \"@HASHTAGTEXT\", @hashTagText\n",
    "    \n",
    "    Example: 'This is my hashtag @mycoolHASHtag. Do you like it too?' -> 'This is my hashtag . Do you like it too?'\n",
    "    \n",
    "    Hint, try to find a way to match any character of the alphabet and then any occurence of these characters.  \n",
    "    \n",
    "* ... removes a URL that uses either the application layer protocol \"http\" or the newer variant \"https\". (Names can include numbers and characters)\n",
    "\n",
    "    Example: \"https://en.wikipedia.org/\", \"http://en.wikipedia.org/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c8b3fa-afbb-437c-a589-489a91ae3863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71e7770b-fe47-473a-ae72-a381ff90de60",
   "metadata": {},
   "source": [
    "## Exercise 1: Tweet normalization \n",
    "We now would like to apply our insights into regular expressions to normalize our data. As a first step, we will write our own simple function based on the regular expressions you just came above with and the examples given above. Afterwards, we will use a more comprehensive pipeline with a larger variety of functions tackling a wider variety of small \"issues\" in our data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6452af-c5f8-429a-9b9b-ca4e2767967f",
   "metadata": {},
   "source": [
    "## Tasks (skip!):\n",
    "\n",
    "* Use the functions provided as examples above (in Exercise 0) that appear useful to normalize our Tweets and the regular expressions you introduced to fulfill the previous tasks to compose a function that normalizes a single Tweet (`ds_tweets['text_original']`).\n",
    "* Write a function that normalizes an array of the first 100 Tweets\n",
    "* Add an additional function to further normalize the data. Discuss your reasoning behind introducing your function with the group!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5d7c0-8b26-4cd1-b449-8b870cb1fba5",
   "metadata": {},
   "source": [
    "## Exercise 2: Tweet normalization pipeline\n",
    "To actually convert our text into a format that is more appropriate for training, we will use functions provided in file `scripts/normalize_text_bootcamp.py` as finding undesired features in our Tweets is quite cumbersome (and may still incomplete in our best current version of the pipeline).\n",
    "\n",
    "In the file, you will find the class `Normalizer` and its method `normalize`, which we will use to normalize the text. As it is a priori unclear if some formating options actually help training the model, there are optional keyword arguments in the normalize function. \n",
    "\n",
    "*Hint*, when using imported scripts that may be changed while you are working on your notebook. You may find it useful to add the following [*magic commands*](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html) \n",
    "\n",
    "```python\n",
    "# allows update of external libraries without need to reload package\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "```\n",
    "\n",
    "at the beggining of your notebook to reload packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd54a83-c68a-4816-9515-f992859ccc51",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "\n",
    "* Write a function that normalizes an array of the first 100 Tweets (quicker to debug than using whole dataset) with the default options given in `normalize`. Use default values.\n",
    "* Use `normalize_text_bootcamp.normalize_text_dataset` to normalize the whole dataset. Use default values.   \n",
    "    *Hint*: Use `ds_tweets.sel(index=slice(0,99))` to only select the first 100 entries in the dataset for more efficient debugging and testing.\n",
    "* The function `normalize_text_bootcamp.normalize_filter_dataset` also filters out 'unwanted' Tweets in addition to normalizing the Tweets via `normalize_text_bootcamp.normalize_text_dataset` (see previous Task). Use it (default values) to normalize and filter your Tweets.\n",
    "* The function `normalize_slang_stopwords` in `scripts/normalize_text_bootcamp.py` hosts a collection of substitutions for colloquial phrases. With the knowledge you gained on regular expressions, add a new substitution to the function that helps us out in the future and share it with the group.  \n",
    "    *Optionally, you could base this substitution on a phrases you find in the \"cleaned\" data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306242c7-ebf1-47ed-8e70-0da35c6c369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows update of external libraries without need to reload package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb1cab-4fb1-4b21-95a3-7bec41fb4de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
