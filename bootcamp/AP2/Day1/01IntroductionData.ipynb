{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7e6aa3-0805-461e-9d8d-27450f349594",
   "metadata": {},
   "source": [
    "# Application 2: Introduction to the data and Xarray\n",
    "To start off, let us have a look at our data, which is comprised of tweets and precipitation at the time of creation and the location of the tweet.\n",
    "To look at the data we will use Xarray. \n",
    "\n",
    "Xarray is a great library to use when analyzing and working with data provided as a multi-dimensional array. A great example for this would be weather data, where much of its relevant data for weather maps like pressure, temperature, precipitation, etc. contains three dimensions, i.e. time, latitude and longitude. \n",
    "\n",
    "In this notebook, we will try to understand the basic features of the [Xarrray api](https://docs.xarray.dev/en/stable/api.html) .\n",
    "\n",
    "To load the data just use `xarray.load_dataset`. The data is located "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c212fec-58c2-4650-b90e-925d334afff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# allows update of external libraries without need to reload package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee977322-5123-4ab7-8022-e86257e14ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65250261-a8d6-4371-9e5f-436f0c8d2978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER_DATA = \"/p/project/deepacf/maelstrom/haque1/dataset/\"\n",
    "# FOLDER_TO_TWEETS = FOLDER_DATA + \"tweets_2017_01_era5_normed.nc\"\n",
    "FOLDER_TO_TWEETS = (\n",
    "    FOLDER_DATA\n",
    "    + \"2017_2020_tweets_rain_sun_vocab_emojis_locations_bba_Tp_era5_no_bots_normalized_filtered_weather_stations_fix_predicted_simpledeberta_radar.nc\"\n",
    ")\n",
    "ds_tweets = xarray.load_dataset(FOLDER_TO_TWEETS)  # load the 2017 subset of the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1710684-b3d9-4e11-919e-b6feb9963048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "#       Optional         #\n",
    "##########################\n",
    "# Get the total number of data points in your dataset\n",
    "total_data_points = ds_tweets.dims[\"index\"]\n",
    "\n",
    "# Calculate the number of data points you want to keep (10% of the total)\n",
    "sample_size = int(0.1 * total_data_points)\n",
    "\n",
    "# Generate random indices for subsampling\n",
    "random_indices = np.random.choice(total_data_points, sample_size, replace=False)\n",
    "\n",
    "# Select the subset of the dataset using the random indices\n",
    "ds_tweets_subsampled = ds_tweets.isel(index=random_indices)\n",
    "\n",
    "# Define the file path for the new\n",
    "output_file_path = (\n",
    "    FOLDER_DATA\n",
    "    + \"20_percent_2017_2020_tweets_rain_sun_vocab_emojis_locations_bba_Tp_era5_no_bots_normalized_filtered_weather_stations_fix_predicted_simpledeberta_radar.nc\"\n",
    ")\n",
    "\n",
    "# Save the subsampled dataset to the new file\n",
    "# Save the subsampled dataset to the new file\n",
    "ds_tweets_subsampled.to_netcdf(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2453a-70fd-4c99-beae-37fbbd4814bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 10% of total data\n",
    "FOLDER_TO_TWEETS = (\n",
    "    FOLDER_DATA\n",
    "    + \"20_percent_2017_2020_tweets_rain_sun_vocab_emojis_locations_bba_Tp_era5_no_bots_normalized_filtered_weather_stations_fix_predicted_simpledeberta_radar.nc\"\n",
    ")\n",
    "ds_tweets = xarray.load_dataset(FOLDER_TO_TWEETS)  # load the 2017 subset of the Tweets 10 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9974a-2fdf-4e63-aaa1-ce241eff00eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed54ba-0cf0-4bdb-86d4-96a4b3a10d60",
   "metadata": {},
   "source": [
    "## Exercise 0: Overview of datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b2b7f3-0448-4dad-be4b-1ea1105330fd",
   "metadata": {},
   "source": [
    "The **Tweets dataset** contains many fields, for us the most interesting will be `text` (text of the Tweets) and `tp` (Total precipitation, which is the accumulated precipitation within an hour in units of metres in depth the water would have if it were spread evenly over the grid box (0.1x0.1 degrees squared) ). The dataset can be found in `/p/project/training2223/a2/data/tweets`.\n",
    "\n",
    "I summarize the meaning of all fields in the following. Most of them are related to Twitter and are also summarized on [Twitter's website](https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all):\n",
    "* Coordinate fields: `index`: Unique index for xarray, which can be used to select Tweets. All other fields are so called data variables, which may become relevant when using xarray.\n",
    "* `text`: **Text of Tweet**. Already preprocessed for training including converting emojis and text normalization.\n",
    "* `text_original`: Raw Tweet data.\n",
    "* `tp_h`: **Total precipitation in meters.** Corresponds to the equivalent height of filling a resolution element (0.1x0.1 square degrees) within an hour. ([see ECMWF webpage for more details](https://apps.ecmwf.int/codes/grib/param-db/?id=228)).\n",
    "* `raining`: **If it is raining** at the Tweets location/time (raining = 1, not raining = 0). Rain is assumed if the total precipitation is above zero and above the nummerical floor (at ~1e-8m).\n",
    "* `created_at`: **Date and time of Tweet creation.**\n",
    "* `time_half`: `created_at` rounded to nearest half past to align precipitation data and Tweets.\n",
    "* `source`: The name of the app the user Tweeted from.\n",
    "* `author_id`: Id of the author.\n",
    "* `id`: Id of the Tweet itself.\n",
    "* Fields related to location: \n",
    "    * `geo.place_id`: Id of the tagged location.\n",
    "    * `geo.coordinates.type`: Type of the location (empty if tagged location).\n",
    "    * `geo.coordinates.coordinates`: Exact GPS location of device when the Tweet was sent (if user opted in).\n",
    "    * `longitude`: Longitude of Tweet (GPS location or center of tagged location).\n",
    "    * `latitude`: Latitude of Tweet (GPS location or center of tagged location).\n",
    "    * `latitude_rounded`: Latitude of Tweet rounded to resolution of precipitation map (0.1 degree) \n",
    "    * `longitude_rounded`: Longitude of Tweet rounded to resolution of precipitation map (0.1 degree) \n",
    "    * `coordinates_estimated`: Location of Tweet estimated from center of tagged location (if tagged = 1, if GPS location available = 0).\n",
    "    * `centroid`: Center of *tagged* location.\n",
    "    * `centroid_longitude`: Longitude of *tagged* location.\n",
    "    * `centroid_latitude`: Latitude of *tagged* location.\n",
    "    * `place_type`: Place type of *tagged* location.\n",
    "    * `bounding_box`: Bounding box of *tagged* location.\n",
    "    * `full_name`: Full name of *tagged* location.\n",
    "    * `bounding_box_area`: Area of central bounding box in squared kilometres.\n",
    "* Fields related to metrics:\n",
    "    * `public_metrics.retweet_count`: Retweet count.\n",
    "    * `public_metrics.reply_count`: Reply count to Tweet.\n",
    "    * `public_metrics.like_count`: Number of counts of Tweet.\n",
    "    * `public_metrics.quote_count`: Number of times this Tweet has been Retweeted with a comment.\n",
    "* `conversation_id`: Id of conversation (Tweets can also be replied for example, so multiple Tweets can generally share the `conversation_id`)\n",
    "* `lang`: Languague of the Tweets\n",
    "* `reply_settings`: Shows who can reply to this Tweet.\n",
    "* `referenced_tweets`: A list of Tweets this Tweet refers to.\n",
    "* `in_reply_to_user_id`: If this Tweet is a Reply, indicates the user ID of the parent Tweet's author. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71fad2-91e1-4cb9-be81-aba89140dfcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load ecmwf forecasting weather model from 2017 for europe\n",
    "FOLDER_DATA = \"/p/project/deepacf/maelstrom/haque1/dataset/\"\n",
    "FOLDER_TO_PRECIPITATION = FOLDER_DATA + \"ds_precipitation_2017.nc\"\n",
    "# FOLDER_TO_PRECIPITATION = (\n",
    "#     \"/p/project/training2223/a2/data/precipitation/ds_precipitation_2017.nc\"\n",
    "# )\n",
    "\n",
    "ds_prec = xarray.load_dataset(FOLDER_TO_PRECIPITATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e118e31-38d9-4d2e-af1f-b38eb0282c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ##########################\n",
    "# #       Optional         #\n",
    "# ##########################\n",
    "# # Calculate the number of data points to keep (10% of the original dataset)\n",
    "# total_data_points = ds_prec.dims['time']\n",
    "\n",
    "# # Calculate the number of data points you want to keep (10% of the total)\n",
    "# sample_size = int(0.1 * total_data_points)\n",
    "\n",
    "# # Generate random indices for subsampling\n",
    "# random_indices = np.random.choice(total_data_points, sample_size, replace=False)\n",
    "\n",
    "# # Select the subset of the dataset using the random indices\n",
    "# ds_prec_subsampled = ds_prec.isel(time=random_indices)\n",
    "\n",
    "# # Now, ds_prec_subsampled contains only 10% of the original dataset\n",
    "# # Define the file path for the new NetCDF file\n",
    "# output_file = FOLDER_DATA + \"ds_precipitation_10_percent.nc\"\n",
    "\n",
    "# # Save the subsampled dataset to the new file\n",
    "# ds_prec_subsampled.to_netcdf(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb1857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER_DATA = \"/p/project/deepacf/maelstrom/haque1/dataset/\"\n",
    "\n",
    "FOLDER_TO_PRECIPITATION = FOLDER_DATA + \"ds_precipitation_10_percent.nc\"\n",
    "# FOLDER_TO_PRECIPITATION = (\n",
    "#     \"/p/project/training2223/a2/data/precipitation/ds_precipitation_2017.nc\"\n",
    "# )\n",
    "\n",
    "ds_prec = xarray.load_dataset(FOLDER_TO_PRECIPITATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d2f783-8e3f-4cae-909d-8e4c9f0e9724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cbbac6-640a-4543-8d15-733e15fd6e2d",
   "metadata": {},
   "source": [
    "In addition, we have the **precipitation datasets** describing total precipitation in Europe from 2017 to 2020 (divided up into one dataset per year) at an hourly temporal resolution and 0.1 x 0.1 square degrees spatial resolution. The datasets can be found in `/p/project/training2223/a2/data/precipitation`.\n",
    "\n",
    "Coordinates:\n",
    "* `time`: Time (always half past)\n",
    "* `latitude`: Latitude\n",
    "* `longitude`: Longitude\n",
    "\n",
    "Data variables:\n",
    "* `tp`: Total precipitation, see above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e5bd1-1e88-474e-8f89-b21d11eb9a50",
   "metadata": {},
   "source": [
    "## Task:\n",
    "* Check the data type of the variables and coordinates in the precipitation dataset `ds_prec`. Do you know how to 'handle' them in python? If not, find out a bit about the 'new' datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6cfba4-6eb5-4332-9976-f77d1713377f",
   "metadata": {},
   "source": [
    "## Exercise 1: Hands on with the precipitation data and introduction to plotting with xarray\n",
    "Let's now look at the data with xarray via some questions regarding the data. To solve the problems, you could plot the data or select the relevant parts of the data. Let's first look at plotting with xarray. Usually plots show up below the executed cell. However, if this is not the case calling `%matplotlib inline` may make them show up again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b57829-560d-4c5b-a186-5653bd5035f8",
   "metadata": {},
   "source": [
    "`ds_tweets` and `ds_prec` are a so called `xarray.Dataset`. Just calling a single variable of the dataset can be done via `ds_prec['tp']` or `ds_prec.tp` like in the popular [pandas](https://pandas.pydata.org/docs/user_guide/index.html) library. Both `ds_prec['tp']` and `ds_prec.tp` are a `xarray.DataArray` which is in internal type of the Xarray library. However, if you just want the raw data of the variable you can just call `ds_prec['tp'].values` and `ds_prec.tp.values`, which gives the variable as a `numpy.array`. Distinguishing between `xarray.Dataset` and `xarray.DataArray` is quite important as many operations are tied to either type so look carefully for the type you are dealing with when checking the [API reference](https://docs.xarray.dev/en/stable/api.html) of Xarray."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf47fa-f285-4427-8f27-630b9d253b24",
   "metadata": {},
   "source": [
    "### 1.1 Plotting a histogram\n",
    "For a simple histogram of a variable in our dataset, we can directly rely on xarrays inbuilt functionality. For example, to plot the distribution of our variable `tp` (total precipitation), we just call `ds_prec.tp.plot.hist()`. However, it seems that the default bins are not really optimized for our data field. To improve on this, we will introduce user-defined bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f7a5c-eb08-4fb4-ad30-13fa51aa817b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "ds_prec.tp.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa68e7-cdc6-43cc-aaf7-01e223e4a545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.min(abs(ds_prec.tp.values[ds_prec.tp.values != 0])), np.max(ds_prec.tp.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ecb5b-5688-4129-8c5e-ba42d14a5b6d",
   "metadata": {},
   "source": [
    "The plotting functions contained in `scipts/plotting` will be useful throughout the tutorials to save time and energy. So, let's use them to compute our desired bins. We realize that we actually have negative values in our dataset. This is of course unphysical but an artifact of how the simulation treats this data field. Due to such a vast range in values that is both positive and negative it probably makes sense to use the 'symmetrical log' (see [this stack overflow question](https://stackoverflow.com/questions/3305865/what-is-the-difference-between-log-and-symlog) for more information on 'symlog'). Fundamentally, it allows you to have logarithmically spaced bins up to the linear threshold, below which a linear scale is used (to prevent divergences at 0).\n",
    "\n",
    "The plotting functionality in xarray relies heavily on [matplotlib](https://matplotlib.org/stable/tutorials/index.html), which you have probably encountered before. The basic concept of matplotlib is that you have a `plt.figure`, which is your canvas or your page on which you are going to plot. But you can of course plot many different plots on a single page, so they introduce a `plt.axes`, which holds a single plot (or other 'plot like' objects like a colorbar). \n",
    "\n",
    "Therefore, after creating our bins, we first initialize a single axes and figure for our plot with the function `plotting.utils_plotting.create_figure_axes` (Remember that you can use the question mark to check arguments and documentation on functions, i.e. `plotting.utils_plotting.create_figure_axes?`). Then we plot our histogram on this axes. Finally, we change the scale of our x-axis to 'symlog' and the scalebar of our y-axis to 'log'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadbd6f5-6d72-4a3c-b785-0654ab833cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f44ea-253c-4eaa-84af-4a1f95e689e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution may take some time ....\n",
    "linear_thresh = 1e-7\n",
    "bins = plotting.histograms.get_bin_edges(\n",
    "    vmin=None,\n",
    "    vmax=None,\n",
    "    linear_thresh=linear_thresh,\n",
    "    n_bins=60,\n",
    "    data=ds_prec.tp.values,\n",
    "    log=\"symlog\",\n",
    ")\n",
    "\n",
    "# initialize a figure `figure` and an axes `ax`\n",
    "figure, ax = plotting.utils_plotting.create_figure_axes()\n",
    "# plot our histogram using our user-defined bins\n",
    "\n",
    "ds_prec.tp.plot.hist(bins=bins, ax=ax)\n",
    "\n",
    "# change the x-axis scale to a symmetrical log scale\n",
    "plotting.utils_plotting.set_x_log(ax=ax, log=\"symlog\", linear_thresh=linear_thresh)\n",
    "\n",
    "# change the y-axis scale to a logarithmic scale\n",
    "plotting.utils_plotting.set_y_log(ax=ax, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b979f-804a-4fc6-842e-c8961e88f3e0",
   "metadata": {},
   "source": [
    "To reduce the dataset according to a certain selection the dataset method `xarray.Dataset.where` can be used. Let's only select data that lies within a cube of sidelength 1 degree latitude and longitude around the Brandenburg gate (52Â°30'58.0\"N 13Â°22'39.1\"E). Longitude and latitude are given in decimal degrees in our dataset, so we can convert the location from DMS (degrees, minutes, seconds) to decimal degrees. You can do this by hand or use an [online GPS converter](https://www.gps-coordinates.net/gps-coordinates-converter), which yields `Latitude = 52.5162804` and `Longitude = 13.3777019`. By default `xarray.Dataset.where` will just mask out all values that do not fulfill the selection and set them to `nan` values. We use `drop=True` to remove them from dataset instead. We store the 'Berlin' dataset to a new variable `ds_berlin`. Afterwards, we plot the distribution of our subselection dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a11acf-579e-4d04-b6ce-d041cbbd3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_berlin = ds_prec.where(\n",
    "    (ds_prec.latitude < 52.5162804 + 0.5)\n",
    "    & (ds_prec.latitude > 52.5162804 - 0.5)\n",
    "    & (ds_prec.longitude > 13.3777019 - 0.5)\n",
    "    & (ds_prec.longitude < 13.3777019 + 0.5),\n",
    "    drop=True,\n",
    ")\n",
    "ds_berlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1cb0e1-a5c0-4db8-9bea-6e99dfc62b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_thresh = 1e-7\n",
    "\n",
    "bins = plotting.histograms.get_bin_edges(\n",
    "    vmin=None,\n",
    "    vmax=None,\n",
    "    linear_thresh=linear_thresh,\n",
    "    n_bins=60,\n",
    "    data=ds_prec.tp.values,\n",
    "    log=\"symlog\",\n",
    ")\n",
    "\n",
    "# initialize a figure `figure` and an axes `ax`\n",
    "figure, ax = plotting.utils_plotting.create_figure_axes()\n",
    "\n",
    "# plot our histogram using our user-defined bins\n",
    "ds_berlin.tp.plot.hist(bins=bins, ax=ax)\n",
    "\n",
    "# change the x-axis scale to a symmetrical log scale\n",
    "plotting.utils_plotting.set_x_log(ax=ax, log=\"symlog\", linear_thresh=linear_thresh)\n",
    "\n",
    "# change the y-axis scale to a logarithmic scale\n",
    "plotting.utils_plotting.set_y_log(ax=ax, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a83ecf-dc23-47a9-98a6-b0d41854a763",
   "metadata": {},
   "source": [
    "### 1.2 Plotting a map\n",
    "Let's now plot a map of the the total precipitation. We will decide on a point in time first, let's use 17:30 in the used 24-hour clock convention (corresponds to 5:30 pm) on 1. July 2017. For this, we need to create a date time object let's use `numpy` as this is the format used by xarray. We can instantiate it by calling ` np.datetime64('2017-07-01T17:30:00')` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e3418a-c9d1-4cf1-96de-6997aa1a4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_prec.sel(time=np.datetime64(\"2017-07-01T17:30:00\")).tp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb63f63-379b-4d68-9b68-76235073d8a7",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "1. Was it raining in the \"Greater London\" area at 2:30pm on 2nd January 2017? \n",
    "\n",
    "    *Hint*, for this task and the following, you can assume that \"Greater London\" is a cube of side length 40 km centered on Waterloo station. \n",
    "2. Was it raining in the greater London area at any time on 2nd January 2017? \n",
    "3. Make a plot of the precipitation data on 2017-07-01 at 12:30 am and zoom onto your favorite European country (arguments `min` and `max` of xarray's `plot` function may come in handy).\n",
    "4. Use a logarithmic colorbar (the keyword `norm` and the matplotlib object `matplotlib.colors.LogNorm`) and change up the colormap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395f4de-20fd-4710-ab7d-df01142df3ec",
   "metadata": {},
   "source": [
    "## Exercise 2: Hands on with the Tweets\n",
    "Now let's turn to the tweets and get a feel for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac912e5-58ac-4e71-a6f1-0da13febf8f3",
   "metadata": {},
   "source": [
    "### Group by in xarray\n",
    "Grouping datasets by a certain value of a variable is a useful tool to have. Let's for example look at the most relevant `place_type`. Unfortunately, this operation can be quite slow especially if many different unique values exist for the `group_by` field .... So we would like to save the result as a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe01cb7-2fa6-47be-953d-22dc9c68ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_dataset(ds, group_by, sort_by=\"id\"):\n",
    "    ds_grouped_unsorted = ds.groupby(group_by).count()\n",
    "    ds_grouped = ds_grouped_unsorted.sortby(sort_by, ascending=False)\n",
    "    return ds_grouped\n",
    "\n",
    "\n",
    "ds_grouped = get_grouped_dataset(ds_tweets, group_by=\"place_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e6a29-7377-4a77-a66b-def5afa2b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ds_grouped[\"place_type\"].values\n",
    "values = ds_grouped[\"id\"].values\n",
    "for k, v in zip(keys, values):\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b683ffe-f303-431c-b1bf-360cced5115a",
   "metadata": {},
   "source": [
    "We notice, that most locations are attributed to 'cities'. The second most relevant `place_type` is an empty string, which is used for places that were not tagged but for which GPUs locations exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac38350-fb92-4889-8a6a-a406bd30a2e3",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "1. By just looking at the Tweets, e.g. the first 100 Tweets (`ds_tweets['text_original'].values[:100]']`) and taking another look at the overview of the Tweet dataset in Exercise 0, try to estimate how they were queried from Twitter (e.g., which keywords used? what properties do the Tweets have?). Let's discuss! \n",
    "2. Let's look at the distribution of the Twitter keywords. Make a histogram of the Tweets containing any of the following keywords. You can find a useful function for this in `plotting.histograms`.\n",
    "3. Find the most crucial sources for the Tweets (Top 10 most used apps to tweet).\n",
    "4. Find something new and interesting about the data and share it with the group!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c5949-04d5-46c0-8ced-d5497d31979e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emojis = [\n",
    "    \"ðŸ”ï¸\",\n",
    "    \"ðŸ”\",\n",
    "    \"â˜€ï¸\",\n",
    "    \"â˜€\",\n",
    "    \"ðŸŒž\",\n",
    "    \"â›…\",\n",
    "    \"â›ˆï¸\",\n",
    "    \"â›ˆ\",\n",
    "    \"ðŸŒ¤ï¸\",\n",
    "    \"ðŸŒ¤\",\n",
    "    \"ðŸŒ¥ï¸\",\n",
    "    \"ðŸŒ¥\",\n",
    "    \"ðŸŒ¦ï¸\",\n",
    "    \"ðŸŒ¦\",\n",
    "    \"ðŸŒ§ï¸\",\n",
    "    \"ðŸŒ§\",\n",
    "    \"ðŸŒ¨ï¸\",\n",
    "    \"ðŸŒ¨\",\n",
    "    \"ðŸŒ©ï¸\",\n",
    "    \"ðŸŒ©\",\n",
    "    \"â˜”\",\n",
    "    \"â›„\",\n",
    "]\n",
    "keywords = emojis + [\n",
    "    \"blizzard\",\n",
    "    \"cloudburst\",\n",
    "    \"downpour\",\n",
    "    \"drizzle\",\n",
    "    \"flash flood\",\n",
    "    \"flood\",\n",
    "    \"flood stage\",\n",
    "    \"forecast\",\n",
    "    \"freezing rain\",\n",
    "    \"hail\",\n",
    "    \"ice storm\",\n",
    "    \"lightning\",\n",
    "    \"precipitation\",\n",
    "    \"rain\",\n",
    "    \"rain gauge\",\n",
    "    \"rain shadow\",\n",
    "    \"rainbands\",\n",
    "    \"rain shower\",\n",
    "    \"snow\",\n",
    "    \"snow shower\",\n",
    "    \"snowstorm\",\n",
    "    \"sun\",\n",
    "    \"sunny\",\n",
    "    \"thunder\",\n",
    "    \"thunderstorm\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425ef77-00de-4089-91df-725a46ba9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tweets.source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa2054-a549-4227-82d8-6047484d8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.histograms.plot_distribution_keywords(ds_tweets.text.values, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff929d-c1f3-4b87-a5f2-65174f24b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e076d-ecd9-4f13-acc6-bf14d63ecc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
