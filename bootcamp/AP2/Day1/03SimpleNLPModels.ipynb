{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303811af-ee61-4287-a509-710776905ed6",
   "metadata": {},
   "source": [
    "# Application 2: Simple NLP machine learning models\n",
    "Now, we look at our first models to classify the data into \"raining\" and \"not raining\". A crucial first step in NLP is always to find numerical representations of our words to use them in a machine learning model. We will encounter two different methods here. As our mode, we rely on a \"simple\" [recurrent neural network](https://d2l.ai/chapter_recurrent-neural-networks/index.html) (RNN) here, which relies on a [Long Short-Term memory (LSTM)](https://d2l.ai/chapter_recurrent-modern/lstm.html) layer. The links provided, give a very concise introduction to RNNs in general and popular models. We won't have time to understand them in detail during the course but feel free to explore the linked sites by yourself. The basic idea behind RNNs is that you make predictions depending on a trained hidden state that works as the \"memory\" of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beefbcd5-b264-4e1a-b82e-c35fe0db0d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows update of external libraries without need to reload package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a7ce6-14bf-4ffd-880f-e458e2faa895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.preprocessing.text\n",
    "from keras.preprocessing import sequence\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import xarray\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "import normalize_text_bootcamp\n",
    "import utils_bootcamp\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf8dbc-4758-4f0e-bd0f-c538de7bb5c5",
   "metadata": {},
   "source": [
    "## Exercise 0: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800325e-962c-48ca-a382-ed6436a93c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_DATA = \"/p/scratch/deepacf/maelstrom/maelstrom_data/ap2/data/tweets/\"\n",
    "FOLDER_DATA = \"/p/project/training2223/a2/data/tweets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d96446-b8d2-4ec9-86fa-9d1c48f476d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_TWEET = FOLDER_DATA + \"tweets_2017_era5_normed_filtered.nc\"\n",
    "# define the variable name of the total precipitation in the dataset\n",
    "key_tp = \"tp_h\"\n",
    "ds = xarray.load_dataset(FOLDER_TWEET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e253d-7b4f-4369-81d3-7f5f3fdf557b",
   "metadata": {},
   "source": [
    "## Exercise 1: Normalization (should already be done at this point)\n",
    "Depending on the state of the data we need to normalize it. It may also make sense to test different possibilities of normalizing the data. Our data is likely already normalized at this point. So, we skip it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a33c2-3e28-4fd6-be89-809954bdce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_norm = normalize_text_bootcamp.normalize_filter_dataset(\n",
    "#     ds,\n",
    "#     keywords=None,\n",
    "#     reset_index=True,\n",
    "#     key_text_original=\"text_original\",\n",
    "#     key_text_normalized=\"text_normalized\",\n",
    "#     key_text_backup=None,\n",
    "#     ignore_non_ascii=True,\n",
    "#     replace_keyword_emojis=True,\n",
    "#     remove_punctuations=\"keep_basic_punctuations\",\n",
    "#     reduce_punctuations=True,\n",
    "#     use_lower_case=True,\n",
    "#     do_split_punctutation_text=False,\n",
    "#     remove_sun_confusing_terms=True,\n",
    "#     only_text_containing_keywords=True,\n",
    "#     maximum_bounding_box_area=100,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfdf7c4-d2b0-446d-aebb-cbfe338fa01a",
   "metadata": {},
   "source": [
    "## Exercise 2: Preparation for training (probably not required)\n",
    "Prepare data for training\n",
    "Before we actually train our model, we need to\n",
    "\n",
    "* define our labels\n",
    "* encode our labels\n",
    "* encode our Tweets into \"numbers\" (vectors)\n",
    "* split our data into a training and test set\n",
    "* define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e15ae2-35a5-4a4c-844f-f01efc3f6190",
   "metadata": {},
   "source": [
    "### Define labels\n",
    "We first define our labels, based on our total precipitation values. By default, we use a threshold of roughly machine precision of the underlying weather forecasting model. However, higher thresholds should be tested in the future to check what Twitter users consider to be \"rain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e8b45d-8819-4d21-a16c-aa316ce90070",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"raining\"] = ([\"index\"], ds[key_tp].values > 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2362da-771e-4e59-8f5f-d1934c1ccf3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define our input and output variables as an array\n",
    "Following the tradition, we define our input variables to the model (normalized text of the Tweets) as `X` and our labels as `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbfe5f1-051b-4ae2-a2ae-4d255599bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds.text_normalized.values\n",
    "Y = ds.raining.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff17a26-79bc-4434-b6f6-3b3cd1ad8f58",
   "metadata": {},
   "source": [
    "### Split training and test set\n",
    "We use `sklearn.model_selection.train_test_split` to split the model.\n",
    "\n",
    "### Task:\n",
    "* We would like to use 80% of the Tweets for training and 20% for testing.\n",
    "* Make sure that both labels are represented in the proportions both in the test and training set. The argument `stratify` of the function `sklearn.model_selection.train_test_split` may help. Check the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f55a55-d68b-40ce-b201-df00fb2d6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=0.15,\n",
    "    # \"stratify= ...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd909ee-5957-41de-93f2-95c012fb5cbd",
   "metadata": {},
   "source": [
    "### Encode our labels \n",
    "This step is required if our label's are in text format (e.g., \"cat\", \"dog\"). This is not the case here, but it's included for completion. The format of encoded labels depends on the model, so you should check them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fb224-df09-4a1d-a049-f7d1f9c9523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced5322d-d6ae-4592-bae8-d0631101cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe97da-67c2-4b15-a643-63ed6b51ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf323d92-de66-41d0-aa59-04ed6e924487",
   "metadata": {},
   "source": [
    "### **Tokenization of the data**\n",
    "Texts have to be broken down into smaller chunks so called \"tokens\", which are then encoded as vectors in the next. Many implementations of tokenization exist. Here, we will use the default `keras` implementation. For a sentence like \"Today will be a sunny day!\", you can imagine just ascribing every individual word a token. However, for a sentence like \"I wouldn't enjoy a sunny day by programming in C++\". Here, tokenizing \"wouldn't\" is not as straightforward. You probably would like to retain the \"would\" and the \"not\" part, so maybe just replacing this word by the long form of the word would be an option. In addition, \"C++\" is by itself a weird combination of a letter and mathematical symbols that you would probably filter out when looking at text. However, the word itself is of course referring to a programming language, which means it should probably be used as a single token. In the next notebook, we will encounter a more complex tokenizer, which is called [sentence-piece](https://jacky2wong.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f79030c-1268-46a4-b5c6-879770fcb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8f52a-e7b2-4ab7-b980-67e9ce61efcd",
   "metadata": {},
   "source": [
    "### **Encoding of the tokens**\n",
    "We now use\n",
    "* the method `fit_on_texts` to connect each token to a unique integer, which will be used for encoding\n",
    "* the method `texts_to_sequences` to encode every Tweet by tokenizing the Tweet and ascribing its unique integer (from the previous step) to the token\n",
    "* the method `pad_sequences` to \"pad\" our encoded Tweets such that they all have the same length (`max_len`), which is required by the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d94340-e5f3-4456-a1d7-59e1f0259a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_matrix = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f15c11-cf80-4b0b-a578-dda9650e8306",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "The model is defined via different layers taken from Keras. [Keras](https://keras.io/getting_started/intro_to_keras_for_researchers/) is a high level API based on TensorFLow. It contains the following layers:\n",
    "\n",
    "* Input layer: accepts our encoded Tweets where each Tweet has the length `max_len`\n",
    "* Embedding layer: trained representation of our encoded Tweets. Currently, our Tweets are encoded as integer values, however neural networks operate with float values usually in the range around -1 to 1, so we would like to present each token as a vector with values in this range. An embedding layer is used for this. We set `output_dim=50`, which means that every token will be represented by a vector of length 50. In addition, its weights (so the representation of individual tokens) are trained when training the model. See [small intro](https://medium.com/analytics-vidhya/understanding-embedding-layer-in-keras-bbe3ff1327ce) or [on stack overflow](https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work/305032#305032).\n",
    "* LSTM layer: The so called Long short-term memory is used in many reccurent neural networks. See links in the introduction to this notebook.\n",
    "* Dense layer: Most basic layer of NN, see [intro here](http://www2.cs.uregina.ca/~dbd/cs831/notes/neural-networks/neural-networks/) for more details.\n",
    "* Activation function: The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.\n",
    "* Dropout layer: The Dropout layer randomly removes neurons from the network with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "* Dense layer: See above\n",
    "* Activation function: sigmoid function, which converts previous outputs to probabilities (add up to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f6e620-e19c-488f-9578-aa7f17b41e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name=\"inputs\", shape=[max_len])\n",
    "    layer = Embedding(max_words, output_dim=50, input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256, name=\"FC1\")(layer)\n",
    "    layer = Activation(\"relu\")(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1, name=\"out_layer\")(layer)\n",
    "    layer = Activation(\"sigmoid\")(layer)\n",
    "    model = Model(inputs=inputs, outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fc1b2-b9b9-4ede-a983-db37372b115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=RMSprop(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e331297-f037-472b-a882-6d983bad3bc1",
   "metadata": {},
   "source": [
    "## Exercise 3: Model training\n",
    "We train our model with a batch size of 128 (amount of data passed simultaneously through the network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f92192-aad1-42f3-b406-0ffc9daa1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    sequences_matrix,\n",
    "    Y_train,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a61bde-9973-41c5-a9e7-55be036be9b4",
   "metadata": {},
   "source": [
    "## Exercise 4: Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5892a5-979a-4413-99ea-09ab8cd44f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build test dataset\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "sequences_matrix_test = pad_sequences(sequences_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ab2f0-8c7f-4f90-bf36-3ce5cc7fbf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(sequences_matrix_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce1e3c6-529b-420b-ada1-275a9790340f",
   "metadata": {},
   "source": [
    "## Further analysis\n",
    "Let's now take a closer look at our results by looking at the so-called [f1-score](https://deepai.org/machine-learning-glossary-and-terms/f-score) and the AUC derived from the [ROC curve](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23c0500-2b0a-4331-a728-d22aef8d1ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = model.predict(sequences_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2695737-8eb0-452c-873b-b3a55692fa84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26730a43-4b67-4c56-b26c-f68d2eda859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.analysis.check_prediction(Y_predict.argmax(-1), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65313f-6715-4e52-8d1b-d7dca4c05820",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.analysis.classification_report(Y_predict.argmax(-1), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab9274-a87c-47cc-85ce-5f87aa5f93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.analysis.plot_roc(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893dc02-3293-4b5c-9836-653501aa256a",
   "metadata": {},
   "source": [
    "### Task:\n",
    "* How come our accuracy on the test set is lower than the accuracy during training?\n",
    "* Overcome this issue by introducing an early stopping mechanisms that stops when our validation accuracy stops improving at the 0.01% level. Use a `callbacks=EarlyStopping(monitor=\"val_loss\", min_delta= ??? )` and `validation_split=0.2` for this.\n",
    "* Why is the precision of \"raining\" zero? Did the model learn something? What do you think is a good measurement of the \"quality\" of our model?\n",
    "* Vary the dropout rate? What happens?\n",
    "* Add more layers to the network. If you do not where to start take a look at alternative LSTMs like [here](https://www.analyticsvidhya.com/blog/2021/06/lstm-for-text-classification/). Can you manage to improve the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d57913-9b86-4330-aa35-fdfb8667a56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap2",
   "language": "python",
   "name": "ap2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
