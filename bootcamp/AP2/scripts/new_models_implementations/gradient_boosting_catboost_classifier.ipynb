{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01183c4e-994f-4479-8129-a5920cfd81fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows update of external libraries without need to reload package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bb0a7-5d39-4010-b23e-6430f51b7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection  # import train_test_split\n",
    "import sklearn.metrics  # import accuracy_score\n",
    "import sklearn.feature_extraction.text  # import CountVectorizer\n",
    "import xgboost  # import XGBClassifier\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "import catboost  # import CatBoostClassifier, Pool\n",
    "\n",
    "import nltk.tokenize  # import word_tokenize\n",
    "import nltk.corpus  # import stopwords\n",
    "import nltk.stem  # import WordNetLemmatizer\n",
    "import nltk.corpus  # import wordnet\n",
    "import nltk.tokenize  # import sent_tokenize\n",
    "import statistics  # import mean\n",
    "import sentence_transformers  # import SentenceTransformer\n",
    "\n",
    "import sklearn.metrics  # import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import tqdm  # import tqdm\n",
    "import matplotlib  # import style\n",
    "\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/p/home/jusers/ehlert1/juwels/notebooks/bootcamp_testing/scripts\")\n",
    "sys.path.append(\"../../scripts\")\n",
    "import normalize_text_bootcamp\n",
    "import utils_bootcamp\n",
    "import dataset_bootcamp\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545c186-5e72-41e2-89b9-12f825a654dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39f989-9955-4c22-ac5c-6961c8635bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "[torch.cuda.device(i) for i in range(torch.cuda.device_count())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6885f34-7bf1-4581-9086-4d2498c24ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f42cc73-0386-471b-b218-6a8cca45e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_TO_TWEETS = \"/p/project/training2223/a2/data/tweets/tweets_2017_normalized_filtered.nc\"\n",
    "FOLDER_TO_TWEETS = \"../../../data/tweets/tweets_2017_normalized_filtered.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3394c2-be4d-4e0c-ad6f-e4b801a7b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tweets = dataset_bootcamp.load_tweets_dataset(FOLDER_TO_TWEETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d8570-8c68-4a4a-ac69-d7fdeb41b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using first 10_001 values to iterate more quickly + resetting index\n",
    "ds_sel = ds_tweets.sel(index=slice(0, 10_000))\n",
    "ds_sel = dataset_bootcamp.reset_index_coordinate(ds_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb058138-8056-4d73-8d17-0f3625a0fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset seems balanced\n",
    "ds_sel.raining.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf36dff-cad0-43f8-b442-159014fd6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds_sel[[\"text_normalized\"]].to_pandas()\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d503b11-469f-45dd-b5c7-0f58aaa7feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a698cb-408a-4c3f-8403-a13c546e8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRANSFORMERS = {\n",
    "    \"sentence-transformers/paraphrase-mpnet-base-v2\": (\"mpnet\", 768),\n",
    "    \"sentence-transformers/bert-base-wikipedia-sections-mean-tokens\": (\n",
    "        \"wikipedia\",\n",
    "        768,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e6b360-60c4-4c00-9e78-fd6d487d5736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode_test(df):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"{device} is used\")\n",
    "    model = sentence_transformers.SentenceTransformer(\n",
    "        \"sentence-transformers/paraphrase-mpnet-base-v2\", cache_folder=f\"./hf_mpnet/\"\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return np.array(model.encode(df[\"text_normalized\"]))\n",
    "\n",
    "\n",
    "# get_encode(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248042e-5397-436f-ae6b-cdf693d2fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode(df, encoder, name):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"{device} is used\")\n",
    "    model = sentence_transformers.SentenceTransformer(encoder, cache_folder=f\"./hf_{name}/\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return np.array(model.encode(df[\"text_normalized\"].to_numpy()))\n",
    "\n",
    "\n",
    "def get_embeddings(df, emb=None, tolist=True):\n",
    "    ret = pd.DataFrame(index=df.index)\n",
    "\n",
    "    for e, s in STRANSFORMERS.items():\n",
    "        if emb and s[0] not in emb:\n",
    "            continue\n",
    "\n",
    "        ret[s[0]] = list(get_encode(df, e, s[0]))\n",
    "        if tolist:\n",
    "            ret = pd.concat(\n",
    "                [\n",
    "                    ret,\n",
    "                    pd.DataFrame(\n",
    "                        ret[s[0]].tolist(),\n",
    "                        columns=[f\"{s[0]}_{x}\" for x in range(s[1])],\n",
    "                        index=ret.index,\n",
    "                    ),\n",
    "                ],\n",
    "                axis=1,\n",
    "                copy=False,\n",
    "                sort=False,\n",
    "            )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    table = text.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "    words = word_tokenize(text.lower().strip().translate(table))\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmed)\n",
    "\n",
    "\n",
    "def get_sentence_lengths(text):\n",
    "    tokened = sent_tokenize(text)\n",
    "    lengths = []\n",
    "\n",
    "    for idx, i in enumerate(tokened):\n",
    "        splited = list(i.split(\" \"))\n",
    "        lengths.append(len(splited))\n",
    "\n",
    "    return (max(lengths), min(lengths), round(mean(lengths), 3))\n",
    "\n",
    "\n",
    "def create_features(df):\n",
    "    df_f = pd.DataFrame(index=df.index)\n",
    "    df_f[\"text_len\"] = df[\"excerpt\"].apply(len)\n",
    "    df_f[\"text_clean_len\"] = df[\"clean_excerpt\"].apply(len)\n",
    "    df_f[\"text_len_div\"] = df_f[\"text_clean_len\"] / df_f[\"text_len\"]\n",
    "    df_f[\"text_word_count\"] = df[\"clean_excerpt\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "    df_f[[\"max_len_sent\", \"min_len_sent\", \"avg_len_sent\"]] = df.apply(\n",
    "        lambda x: get_sentence_lengths(x[\"excerpt\"]), axis=1, result_type=\"expand\"\n",
    "    )\n",
    "\n",
    "    return df_f\n",
    "\n",
    "\n",
    "def getWordsFromURL(url):\n",
    "    return re.compile(r\"[\\:/?=\\-&.]+\", re.UNICODE).split(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb04cb9-bc6d-4ccc-b9ca-106be888bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpo = {\n",
    "    \"tokenizers\": [\n",
    "        {\n",
    "            \"tokenizer_id\": \"Sense\",\n",
    "            \"separator_type\": \"BySense\",\n",
    "            \"lowercasing\": \"True\",\n",
    "            \"token_types\": [\"Word\", \"Number\"],\n",
    "        }\n",
    "    ],\n",
    "    \"dictionaries\": [\n",
    "        {\n",
    "            \"dictionary_id\": \"Word\",\n",
    "            \"token_level_type\": \"Word\",\n",
    "            \"occurrence_lower_bound\": \"2\",\n",
    "        },\n",
    "        {\n",
    "            \"dictionary_id\": \"Bigram\",\n",
    "            \"token_level_type\": \"Word\",\n",
    "            \"gram_order\": \"2\",\n",
    "            \"occurrence_lower_bound\": \"2\",\n",
    "        },\n",
    "        {\n",
    "            \"dictionary_id\": \"Trigram\",\n",
    "            \"token_level_type\": \"Word\",\n",
    "            \"gram_order\": \"3\",\n",
    "            \"occurrence_lower_bound\": \"2\",\n",
    "        },\n",
    "    ],\n",
    "    \"feature_processing\": {\n",
    "        \"0\": [\n",
    "            {\n",
    "                \"tokenizers_names\": [\"Sense\"],\n",
    "                \"dictionaries_names\": [\"Word\"],\n",
    "                \"feature_calcers\": [\"BoW\"],\n",
    "            },\n",
    "            {\n",
    "                \"tokenizers_names\": [\"Sense\"],\n",
    "                \"dictionaries_names\": [\"Bigram\"],\n",
    "                \"feature_calcers\": [\"BoW\"],\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "tpo_2 = {\n",
    "    \"tokenizers\": [\n",
    "        {\n",
    "            \"tokenizer_id\": \"Sense\",\n",
    "            \"separator_type\": \"BySense\",\n",
    "            \"lowercasing\": \"True\",\n",
    "            \"token_types\": [\"Word\", \"Number\"],\n",
    "        }\n",
    "    ],\n",
    "    \"dictionaries\": [\n",
    "        {\n",
    "            \"dictionary_id\": \"Word\",\n",
    "            \"token_level_type\": \"Word\",\n",
    "            \"occurrence_lower_bound\": \"2\",\n",
    "        },\n",
    "        {\n",
    "            \"dictionary_id\": \"Bigram\",\n",
    "            \"token_level_type\": \"Word\",\n",
    "            \"gram_order\": \"2\",\n",
    "            \"occurrence_lower_bound\": \"2\",\n",
    "        },\n",
    "        {\n",
    "            \"dictionary_id\": \"Trigram\",\n",
    "            \"token_level_type\": \"Word\",\n",
    "            \"gram_order\": \"3\",\n",
    "            \"occurrence_lower_bound\": \"2\",\n",
    "        },\n",
    "    ],\n",
    "    \"feature_processing\": {\n",
    "        \"0\": [\n",
    "            {\n",
    "                \"tokenizers_names\": [\"Sense\"],\n",
    "                \"dictionaries_names\": [\"Word\"],\n",
    "                \"feature_calcers\": [\"BoW\", \"BM25\"],\n",
    "            },\n",
    "            {\n",
    "                \"tokenizers_names\": [\"Sense\"],\n",
    "                \"dictionaries_names\": [\"Bigram\", \"Trigram\"],\n",
    "                \"feature_calcers\": [\"BoW\"],\n",
    "            },\n",
    "        ],\n",
    "        \"1\": [\n",
    "            {\n",
    "                \"tokenizers_names\": [\"Sense\"],\n",
    "                \"dictionaries_names\": [\"Word\"],\n",
    "                \"feature_calcers\": [\"BoW\"],\n",
    "            },\n",
    "            {\n",
    "                \"tokenizers_names\": [\"Sense\"],\n",
    "                \"dictionaries_names\": [\"Bigram\"],\n",
    "                \"feature_calcers\": [\"BoW\"],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b62c8-34b7-48c6-954b-a9b321c071b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_classifier(train_pool, test_pool, **kwargs):\n",
    "    model = catboost.CatBoostClassifier(\n",
    "        iterations=5000,\n",
    "        eval_metric=\"AUC\",\n",
    "        od_type=\"Iter\",\n",
    "        od_wait=500,\n",
    "        l2_leaf_reg=10,\n",
    "        bootstrap_type=\"Bernoulli\",\n",
    "        subsample=0.7,\n",
    "        thread_count=20,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    return model.fit(train_pool, eval_set=test_pool, verbose=100, plot=False, use_best_model=True)\n",
    "\n",
    "\n",
    "def get_oof_classifier(\n",
    "    n_folds,\n",
    "    x_train,\n",
    "    y,\n",
    "    embedding_features,\n",
    "    cat_features,\n",
    "    text_features,\n",
    "    tpo,\n",
    "    seeds,\n",
    "    num_bins,\n",
    "    emb=None,\n",
    "    tolist=True,\n",
    "    gpu=True,\n",
    "):\n",
    "    ntrain = x_train.shape[0]\n",
    "\n",
    "    oof_train = np.zeros((len(seeds), ntrain, num_bins))\n",
    "    models = {}\n",
    "    for iseed, seed in enumerate(seeds):\n",
    "        kf = sklearn.model_selection.StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "        for i, (tr_i, t_i) in enumerate(kf.split(x_train, y)):\n",
    "            print(f\"tr_i:{tr_i}, t_i:{t_i}\")\n",
    "            if emb and len(emb) > 0:\n",
    "                x_tr = pd.concat(\n",
    "                    [\n",
    "                        x_train.iloc[tr_i, :],\n",
    "                        get_embeddings(x_train.iloc[tr_i, :], emb, tolist),\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                    copy=False,\n",
    "                    sort=False,\n",
    "                )\n",
    "                x_te = pd.concat(\n",
    "                    [\n",
    "                        x_train.iloc[t_i, :],\n",
    "                        get_embeddings(x_train.iloc[t_i, :], emb, tolist),\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                    copy=False,\n",
    "                    sort=False,\n",
    "                )\n",
    "                columns = [x for x in x_tr if (x not in [\"text_normalized\"])]\n",
    "                if not embedding_features:\n",
    "                    for c in emb:\n",
    "                        columns.remove(c)\n",
    "            else:\n",
    "                x_tr = x_train.iloc[tr_i, :]\n",
    "                x_te = x_train.iloc[t_i, :]\n",
    "                columns = [x for x in x_tr if (x not in [\"text_normalized\"])]\n",
    "            x_tr = x_tr[columns]\n",
    "            x_te = x_te[columns]\n",
    "            y_tr = y[tr_i]\n",
    "            y_te = y[t_i]\n",
    "            train_pool = catboost.Pool(\n",
    "                data=x_tr,\n",
    "                label=y_tr,\n",
    "                cat_features=cat_features,\n",
    "                embedding_features=embedding_features,\n",
    "                text_features=text_features,\n",
    "            )\n",
    "            valid_pool = catboost.Pool(\n",
    "                data=x_te,\n",
    "                label=y_te,\n",
    "                cat_features=cat_features,\n",
    "                embedding_features=embedding_features,\n",
    "                text_features=text_features,\n",
    "            )\n",
    "            task_type = \"GPU\" if gpu else \"CPU\"\n",
    "            model = fit_model_classifier(\n",
    "                train_pool,\n",
    "                valid_pool,\n",
    "                random_seed=seed,\n",
    "                task_type=task_type,\n",
    "                text_processing=tpo,\n",
    "            )\n",
    "            oof_train[iseed, t_i, :] = model.predict_proba(valid_pool)\n",
    "            models[(seed, i)] = model\n",
    "\n",
    "    oof_train = oof_train.mean(axis=0)\n",
    "\n",
    "    return oof_train, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7488c-f751-4815-bf4d-4c3286b86901",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "for gpu in [False]:\n",
    "    postfix = \"_gpu\" if gpu else \"\"\n",
    "\n",
    "    params.update(\n",
    "        {\n",
    "            \"emb_basic_f_columns\"\n",
    "            + postfix: {\n",
    "                \"x_train\": df,\n",
    "                \"embedding_features\": [\"mpnet\", \"wikipedia\"],\n",
    "                \"text_features\": None,\n",
    "                \"tpo\": tpo,\n",
    "                \"emb\": [\"mpnet\", \"wikipedia\"],\n",
    "                \"tolist\": False,\n",
    "                \"gpu\": gpu,\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "results = {}\n",
    "\n",
    "for k, v in params.items():\n",
    "    results[k] = get_oof_classifier(\n",
    "        n_folds=FOLDS, y=ds_sel[\"raining\"].values, cat_features=None, seeds=[0, 42, 888], num_bins=2, **v\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f71be-a432-45da-b580-2cbdeb58fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "v[0].argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d7412-148e-41a3-b526-27846c30b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for k, v in results.items():\n",
    "    auc = sklearn.metrics.roc_auc_score(ds_sel[\"raining\"].values, v[0].argmax(-1))  # , multi_class=\"ovo\")\n",
    "    if \"gpu\" in str(k):\n",
    "        name = k[0:-4]\n",
    "        # auc_gpu = sklearn.metrics.roc_auc_score(ds_sel['raining'].values, v[0].argmax(-1), multi_class=\"ovo\")\n",
    "    else:\n",
    "        # auc_cpu = sklearn.metrics.roc_auc_score(ds_sel['raining'].values, v[0].argmax(-1), multi_class=\"ovo\")\n",
    "        name = k\n",
    "    if name in res:\n",
    "        res[name] = res[name] + [auc]\n",
    "    else:\n",
    "        res[name] = [auc]\n",
    "pd.DataFrame.from_dict(res, orient=\"index\", columns=[\"AUC(CPU)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343fc51e-aad0-4795-8317-f54f0a04fd28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
