{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189d620-3fe8-42c1-a075-87dc583e796d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc47c7c-242c-4184-9fa3-3ed6773eb3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"/p/project/deepacf/maelstrom/ehlert1/models/falcon-40b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6689659-c217-4883-9983-b493527c5f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, device_map=\"auto\", trust_remote_code=False, quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f70424-8037-4cdc-83c9-d4a50910b31b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_prompt(prompt):\n",
    "    return tokenizer.encode(prompt, return_tensors=\"pt\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f628f-1028-4d94-8ee3-870149c82455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a45d7-ef73-4315-931f-f2f2d039a128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the prompt\n",
    "prompt = r\"\"\"\n",
    "Does the following sentence provide information on presence of rain? Explain your reasoning.\n",
    "\n",
    "Sentence: It is raining in London.\n",
    "\"\"\"\n",
    "input_ids = tokenize_prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2f2cc6-635d-46e4-9bb6-787b801e3146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sequences = model.generate(\n",
    "    input_ids,\n",
    "    temperature=0.7,\n",
    "    # do_sample=True,\n",
    "    max_length=100,\n",
    "    # top_k=50,\n",
    "    # top_p=0.95,\n",
    "    # num_return_sequences=3\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660b3fc4-5fc0-4665-8c3f-96d6ceb9f0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the results\n",
    "for i, sample_output in enumerate(sequences):\n",
    "    prediction = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    print(f\"{prompt=}\")\n",
    "    print(f\"---------\")\n",
    "    print(f\"prediction\\n{prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f2fdff-fd81-43c4-a384-8090c0518de0",
   "metadata": {},
   "source": [
    "# Tweets in CSV file\n",
    "skip this if you have run that once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36168e6-c0d0-4041-87d5-d7074c2b7fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import a2.utils\n",
    "\n",
    "import a2.training.training_hugging\n",
    "import a2.training.evaluate_hugging\n",
    "import a2.training.dataset_hugging\n",
    "import a2.plotting.analysis\n",
    "import a2.plotting.histograms\n",
    "import a2.dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d282f5a-c390-4872-af7c-72893ac1be43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER_DATA = \"/p/project/deepacf/maelstrom/haque1/dataset/\"\n",
    "FILE_TWEETS = FOLDER_DATA + \"tweets_2017_01_era5_normed_filtered.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a6f91-eaf5-4407-ac69-d57d7b11abf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = a2.dataset.load_dataset.load_tweets_dataset(FILE_TWEETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa2165-4656-4a3b-8cc5-584acfcebe75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds[\"relevance_hand\"] = ([\"index\"], np.ones_like(ds.index.values))\n",
    "ds[[\"text\", \"raining\", \"raining_station\", \"relevance_hand\"]].to_pandas().to_csv(\n",
    "    \"tweets_2017_01_era5_normed_filtered.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae5806e-156a-44d4-bfbe-6ee985dfdd14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9130e84-cde4-48cd-a4a3-77c846da6e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open(\"tweets_2017_01_era5_normed_filtered.csv\", \"r\")\n",
    "file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05099e-bf72-451e-8eca-9f4e34b9d3ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file[\"raining\"] = ([\"index\"], np.array(ds.tp_mm_station.values > 6e-3, dtype=int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa77e83-01dd-49bb-a979-7af35b139a53",
   "metadata": {},
   "source": [
    "# Running model with different tweet numbers to check if this makes any effect with time efficiency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66986463-0f30-40d3-a08f-0f7754c66baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aef582-206b-4c85-bc9b-0cbf1147dbae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to generate Fibonacci numbers\n",
    "def fibonacci(n):\n",
    "    fib_nums = [0, 1]\n",
    "    while fib_nums[-1] < n:\n",
    "        fib_nums.append(fib_nums[-1] + fib_nums[-2])\n",
    "    return fib_nums[1:-1]\n",
    "\n",
    "\n",
    "# Read CSV data\n",
    "df = pd.read_csv(\"tweets_2017_01_era5_normed_filtered.csv\")\n",
    "\n",
    "\n",
    "# Placeholder function for model.generate\n",
    "def generate(input_ids, temperature, max_length):\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids,\n",
    "        temperature=0.7,\n",
    "        # do_sample=True,\n",
    "        max_length=max_length,\n",
    "        # top_k=50,\n",
    "        # top_p=0.95,\n",
    "        # num_return_sequences=3\n",
    "    )\n",
    "    return sample_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac7f99-0be6-4681-82fb-c17dee4d5059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_predictions(seq, prompt, tokenizer):\n",
    "    \"\"\"\n",
    "    Prints the prompt and predictions for each sequence in the generated output.\n",
    "\n",
    "    Args:\n",
    "    sequence (list): A list of generated sequences from the model.\n",
    "    prompt (str): The original prompt used for generation.\n",
    "    tokenizer: The tokenizer used for decoding the sequences.\n",
    "    \"\"\"\n",
    "    for i, sample_output in enumerate(seq):\n",
    "        prediction = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "        print(f\"Prompt:\\n{prompt}\")\n",
    "        print(\"---------\")\n",
    "        print(f\"Prediction {i+1}:\\n{prediction}\\n\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'sequence' is your list of generated outputs and 'tokenizer' is your decoding tokenizer\n",
    "# print_predictions(sequence, prompt, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdcfc0-1dca-427d-a488-47957b08fb65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Record time for each Fibonacci number of rows\n",
    "times = []\n",
    "start_times = []\n",
    "end_times = []\n",
    "tweet_counts = []\n",
    "results = \"\"\n",
    "fib_numbers = fibonacci(len(df))\n",
    "for fib_num in fib_numbers:\n",
    "    if fib_num > 1:  # Limit to 1000 rows\n",
    "        break\n",
    "    # if fib_num < 500:  # Limit to 1000 rows\n",
    "    #     continue\n",
    "\n",
    "    # Prepare the prompt with the selected number of tweets\n",
    "\n",
    "    # Adjusted code to format the prompt as per the example\n",
    "    prompt = \"\"\"\n",
    "    Read below Tweets and tell me if they say that it is raining or sunny. It should be rainy or sunny now.\n",
    "\n",
    "    Tweets:\n",
    "    \"\"\"\n",
    "\n",
    "    for i, row in df.iloc[:fib_num].iterrows():\n",
    "        prompt += f\"\"\"Tweet {i+1}: \"{row['text']}\"\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tokenize_prompt(prompt)\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate the sequence\n",
    "    # sequence = generate(input_ids, temperature=0.7, max_length=len(input_ids))\n",
    "    sequences = model.generate(\n",
    "        input_ids,\n",
    "        temperature=0.7,\n",
    "        # do_sample=True,\n",
    "        max_length=len(prompt) * 3\n",
    "        # top_k=50,\n",
    "        # top_p=0.95,\n",
    "        # num_return_sequences=3\n",
    "    )\n",
    "\n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate and store the time taken\n",
    "    times.append(end_time - start_time)\n",
    "    start_times.append(start_time)\n",
    "    end_times.append(end_time)\n",
    "    tweet_counts.append(fib_num)\n",
    "    results = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700bbcd-99b1-4472-b2b8-ee9b5a68acd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fib_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58167401-de68-46c7-8fa9-73611b7ecf4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "times, tweet_counts, start_times, end_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b2996-3d11-4c12-b80d-b87b0d80d413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the results\n",
    "for i, sample_output in enumerate(results):\n",
    "    prediction = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    print(f\"{prompt=}\")\n",
    "    print(f\"---------\")\n",
    "    print(f\"prediction\\n{prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9cb924-8ed6-4789-95a0-18ff9cad33d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the line plot\n",
    "plt.plot(tweet_counts, times, marker=\"o\")\n",
    "plt.xlabel(\"Number of Tweets\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"Processing Time vs. Number of Tweets (Line Plot)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb44666-28ed-4f63-a943-e3385e94b5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the line plot\n",
    "plt.plot(start_times, end_times, marker=\"o\")\n",
    "plt.xlabel(\"Number of Tweets\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"Processing Time vs. Number of Tweets (Line Plot)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1161877-702b-40cb-9078-1dbe19c855de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap2falcon",
   "language": "python",
   "name": "ap2falcon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
