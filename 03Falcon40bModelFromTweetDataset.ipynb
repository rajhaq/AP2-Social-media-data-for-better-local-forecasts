{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffa59cc",
   "metadata": {},
   "source": [
    "# Falcon 40B Model Test: from tweets_2017_01_era5_normed_filtered dataset\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this analysis, we focus on converting a dataset to CSV format and assessing the time efficiency of a language model when tested with varying numbers of tweets. The process involves transforming a dataset, likely in a structured xarray, into a CSV file. Subsequently, we systematically test the language model's performance by providing different batch sizes of tweets as input.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Dataset Conversion:** The initial step involves converting a dataset, potentially in the xarray format, to CSV. \n",
    "\n",
    "2. **Language Model Testing:** A language model is then tested with different numbers of tweets to evaluate its time efficiency. The goal is to observe how the processing time varies with an increasing number of tweets in the input batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189d620-3fe8-42c1-a075-87dc583e796d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import a2.utils\n",
    "\n",
    "import a2.training.training_hugging\n",
    "import a2.training.evaluate_hugging\n",
    "import a2.training.dataset_hugging\n",
    "import a2.plotting.analysis\n",
    "import a2.plotting.histograms\n",
    "import a2.dataset\n",
    "\n",
    "sys.path.append(\"../scripts/plotting\")\n",
    "import plot_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc47c7c-242c-4184-9fa3-3ed6773eb3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"/p/project/deepacf/maelstrom/ehlert1/models/falcon-40b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6689659-c217-4883-9983-b493527c5f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, device_map=\"auto\", trust_remote_code=False, quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f70424-8037-4cdc-83c9-d4a50910b31b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_prompt(prompt):\n",
    "    return tokenizer.encode(prompt, return_tensors=\"pt\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f628f-1028-4d94-8ee3-870149c82455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f2fdff-fd81-43c4-a384-8090c0518de0",
   "metadata": {},
   "source": [
    "# Tweets in CSV file\n",
    "skip this if you have run that once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d282f5a-c390-4872-af7c-72893ac1be43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER_DATA = \"/p/project/deepacf/maelstrom/haque1/dataset/\"\n",
    "FILE_TWEETS = FOLDER_DATA + \"tweets_2017_01_era5_normed_filtered.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a6f91-eaf5-4407-ac69-d57d7b11abf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = a2.dataset.load_dataset.load_tweets_dataset(FILE_TWEETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa2165-4656-4a3b-8cc5-584acfcebe75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds[\"relevance_hand\"] = ([\"index\"], np.ones_like(ds.index.values))\n",
    "ds[[\"text\", \"raining\", \"raining_station\", \"relevance_hand\"]].to_pandas().to_csv(\n",
    "    \"tweets_2017_01_era5_normed_filtered.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9130e84-cde4-48cd-a4a3-77c846da6e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open(\"tweets_2017_01_era5_normed_filtered.csv\", \"r\")\n",
    "file = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa77e83-01dd-49bb-a979-7af35b139a53",
   "metadata": {},
   "source": [
    "# Running model with different tweet numbers to check if this makes any effect with time efficiency \n",
    "## Overview\n",
    "\n",
    "In this script, we aim to analyze the time efficiency of a language model when generating sequences based on different numbers of tweets. The model is tasked with evaluating whether the given tweets indicate current weather conditions as either rainy or sunny. To measure efficiency, we took different batches of tweets, and the random numbers for these batches were generated using a Fibonacci series.\n",
    "\n",
    "The script iterates through Fibonacci numbers, preparing a prompt with a corresponding number of tweets from a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aef582-206b-4c85-bc9b-0cbf1147dbae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to generate Fibonacci numbers\n",
    "def fibonacci(n):\n",
    "    fib_nums = [0, 1]\n",
    "    while fib_nums[-1] < n:\n",
    "        fib_nums.append(fib_nums[-1] + fib_nums[-2])\n",
    "    return fib_nums[1:-1]\n",
    "\n",
    "\n",
    "# Read CSV data\n",
    "df = pd.read_csv(\"tweets_2017_01_era5_normed_filtered.csv\")\n",
    "\n",
    "\n",
    "# Placeholder function for model.generate\n",
    "def generate(input_ids, temperature, max_length):\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids,\n",
    "        temperature=0.7,\n",
    "        # do_sample=True,\n",
    "        max_length=max_length,\n",
    "        # top_k=50,\n",
    "        # top_p=0.95,\n",
    "        # num_return_sequences=3\n",
    "    )\n",
    "    return sample_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac7f99-0be6-4681-82fb-c17dee4d5059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_predictions(seq, prompt, tokenizer):\n",
    "    \"\"\"\n",
    "    Prints the prompt and predictions for each sequence in the generated output.\n",
    "\n",
    "    Args:\n",
    "    sequence (list): A list of generated sequences from the model.\n",
    "    prompt (str): The original prompt used for generation.\n",
    "    tokenizer: The tokenizer used for decoding the sequences.\n",
    "    \"\"\"\n",
    "    for i, sample_output in enumerate(seq):\n",
    "        prediction = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "        print(f\"Prompt:\\n{prompt}\")\n",
    "        print(\"---------\")\n",
    "        print(f\"Prediction {i+1}:\\n{prediction}\\n\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'sequence' is your list of generated outputs and 'tokenizer' is your decoding tokenizer\n",
    "# print_predictions(sequence, prompt, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdcfc0-1dca-427d-a488-47957b08fb65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Record time for each Fibonacci number of rows\n",
    "times = []\n",
    "start_times = []\n",
    "end_times = []\n",
    "tweet_counts = []\n",
    "results = \"\"\n",
    "fib_numbers = fibonacci(len(df))\n",
    "for fib_num in fib_numbers:\n",
    "    if fib_num > 1:  # Limit to 1000 rows\n",
    "        break\n",
    "    # if fib_num < 500:  # Limit to 1000 rows\n",
    "    #     continue\n",
    "\n",
    "    # Prepare the prompt with the selected number of tweets\n",
    "\n",
    "    # Adjusted code to format the prompt as per the example\n",
    "    prompt = \"\"\"\n",
    "    Read below Tweets and tell me if they say that it is raining or sunny. It should be rainy or sunny now.\n",
    "\n",
    "    Tweets:\n",
    "    \"\"\"\n",
    "\n",
    "    for i, row in df.iloc[:fib_num].iterrows():\n",
    "        prompt += f\"\"\"Tweet {i+1}: \"{row['text']}\"\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tokenize_prompt(prompt)\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate the sequence\n",
    "    # sequence = generate(input_ids, temperature=0.7, max_length=len(input_ids))\n",
    "    sequences = model.generate(\n",
    "        input_ids,\n",
    "        temperature=0.7,\n",
    "        # do_sample=True,\n",
    "        max_length=len(prompt) * 3\n",
    "        # top_k=50,\n",
    "        # top_p=0.95,\n",
    "        # num_return_sequences=3\n",
    "    )\n",
    "\n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate and store the time taken\n",
    "    times.append(end_time - start_time)\n",
    "    start_times.append(start_time)\n",
    "    end_times.append(end_time)\n",
    "    tweet_counts.append(fib_num)\n",
    "    results = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b2996-3d11-4c12-b80d-b87b0d80d413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the results\n",
    "for i, sample_output in enumerate(results):\n",
    "    prediction = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    print(f\"{prompt=}\")\n",
    "    print(f\"---------\")\n",
    "    print(f\"prediction\\n{prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9cb924-8ed6-4789-95a0-18ff9cad33d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the line plot\n",
    "plot_line(tweet_counts, times, \"Number of Tweets\", \"Time (seconds)\", \"Processing Time vs. Number of Tweets (Line Plot)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb44666-28ed-4f63-a943-e3385e94b5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the line plot\n",
    "plot_line(\n",
    "    start_times, end_times, \"Number of Tweets\", \"Time (seconds)\", \"Processing Time vs. Number of Tweets (Line Plot)\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e69cf0",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The analysis of language model performance based on different numbers of tweets yielded insightful results. The script iterated through Fibonacci numbers, each representing a batch of tweets for evaluation. The random numbers generated in a Fibonacci series allowed us to vary the size of the tweet dataset.\n",
    "\n",
    "### Observations:\n",
    "\n",
    "- **Time Efficiency:** As expected, the runtime for sequence generation increased with a higher number of tweets in the batch. This observation suggests a correlation between the number of input tweets and the processing time required by the language model.\n",
    "\n",
    "- **Fibonacci Series:** Using a Fibonacci series for random numbers provided a structured approach to varying tweet batch sizes. This approach allowed for a systematic exploration of different input scenarios.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap2falcon",
   "language": "python",
   "name": "ap2falcon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
