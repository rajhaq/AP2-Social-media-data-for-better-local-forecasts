{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our Baseline Relevance Classifier Model with New Dataset (DeBERTa)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook trains a relevance classifier using the [DeBERTa small](https://huggingface.co/microsoft/deberta-v3-small/tree/main) model, applied to a new dataset generated by the Falcon 40B model from the ER5 tweet dataset. The data is formatted as a CSV file, from our methodology from the Bootcamp day 2 reference, 04ModelTrelevance.ipynb.\n",
    "\n",
    "## Key Steps and Objectives\n",
    "\n",
    "1. **Classifier Training**: We fine-tune the DeBERTa small model on the Falcon 40B dataset to adapt it to the specific task of classifying the relevance of Tweets.\n",
    "2. **Dataset Generation**:\n",
    "   - **Threshold for Relevance**: Tweets are considered relevant if their score is above `0.5`.\n",
    "   - **Link to Notebook**: [Notebook for Generating Tweets](https://github.com/rajhaq/AP2-Social-media-data-for-better-local-forecasts/blob/main/04Falcon40bModelFromTweetDatasetBatchCode.ipynb)\n",
    "   - **Relevant vs. Not Relevant Ratio**: The ratio between relevant and not relevant Tweets is `2.09:1`.\n",
    "3. **Performance of the Model**:\n",
    "   - **Training and Test Sets**:\n",
    "     - Tweets in the training set constitute 80%(`16966`) of the total `25080` tweets, while the test set comprises the remaining 20%(`8114`).\n",
    "   - **ROC Score**: The ROC score of the model is `0.6414`.\n",
    "   - **F1 Scores**:\n",
    "     - F1 score for relevant tweets: `0.7261`\n",
    "     - F1 score for not relevant tweets: `0.0`\n",
    "\n",
    "This notebook is essential for evaluating the DeBERTa-based relevance classifier on the new dataset and offers comprehensive insights through detailed visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import transformers\n",
    "import datasets\n",
    "import functools\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\n",
    "    \"/p/project/deepacf/maelstrom/haque1/AP2-Social-media-data-for-better-local-forecasts/bootcamp/AP2/scripts\"\n",
    ")\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Load the dataset\n",
    "folder_path = \"/p/project/deepacf/maelstrom/haque1/dataset/\"\n",
    "file = \"tweets_2017_01_02_03_era5_normed_no_snow.csv\"\n",
    "file_path = folder_path + file\n",
    "ds_tweets = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again define labels\n",
    "key_input = \"content\"\n",
    "ds_tweets[\"content\"] = ds_tweets[key_input].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ds_tweets[\"relevance\"].astype(int)\n",
    "# Split the indices into training and testing sets\n",
    "indices_train, indices_test = train_test_split(ds_tweets.index, test_size=0.20, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained tokenizer\n",
    "model_nm = \"/p/project/deepacf/maelstrom/haque1/deberta-v3-small\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_nm)\n",
    "db_config_base = transformers.AutoConfig.from_pretrained(model_nm, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(x, tokenizer):\n",
    "    return tokenizer(x[\"inputs\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "# Function to convert the dataset to a format used by Hugging Face\n",
    "def get_dataset_from_csv(ds, tok_func, tokenizer, indices_train, indices_test, train=True):\n",
    "    df = ds.rename(columns={\"content\": \"inputs\", \"relevance\": \"labels\"})\n",
    "    datasets_ds = datasets.Dataset.from_pandas(df)\n",
    "    tok_function_partial = functools.partial(tok_func, tokenizer=tokenizer)\n",
    "    tok_ds = datasets_ds.map(tok_function_partial, batched=True)\n",
    "    if train:\n",
    "        return datasets.DatasetDict({\"train\": tok_ds.select(indices_train), \"test\": tok_ds.select(indices_test)})\n",
    "    else:\n",
    "        return tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = get_dataset_from_csv(ds_tweets, tok_func, tokenizer, indices_train, indices_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_output = \"./outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(params, db_config_base, model_nm):\n",
    "    db_config = db_config_base\n",
    "    if params is not None:\n",
    "        db_config.update({\"cls_dropout\": params[\"cls_dropout\"]})\n",
    "    db_config.update({\"num_labels\": 2})\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(model_nm, config=db_config)\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    classification_report = sklearn.metrics.classification_report(\n",
    "        labels, predictions, target_names=[\"not relevance\", \"relevance\"], output_dict=True\n",
    "    )\n",
    "    f1_not_relevance = classification_report[\"not relevance\"][\"f1-score\"]\n",
    "    f1_relevance = classification_report[\"relevance\"][\"f1-score\"]\n",
    "    return {\"f1_not_relevance\": f1_not_relevance, \"f1_relevance\": f1_relevance}\n",
    "\n",
    "\n",
    "def get_trainer(dataset, db_config_base, model_nm, folder_to_output, parameters):\n",
    "    args = transformers.TrainingArguments(\n",
    "        folder_to_output,\n",
    "        learning_rate=parameters[\"learning_rate\"],\n",
    "        warmup_ratio=parameters[\"warmup_ratio\"],\n",
    "        lr_scheduler_type=parameters[\"lr_scheduler_type\"],\n",
    "        disable_tqdm=False,\n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=parameters[\"batch_size\"],\n",
    "        per_device_eval_batch_size=parameters[\"batch_size\"],\n",
    "        num_train_epochs=parameters[\"epochs\"],\n",
    "        weight_decay=parameters[\"weight_decay\"],\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    get_model_partial = functools.partial(get_model, db_config_base=db_config_base, model_nm=model_nm)\n",
    "    return transformers.Trainer(\n",
    "        model_init=get_model_partial,\n",
    "        args=args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters\n",
    "Default hyperparameters from `bootcamp>Day2>04ModelTraining`. In the following we will define our default values for our hyper parameters. Some of these parameters are model independent and others specific to the model. While documentation for model independent parameters can found [here](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments), model dependant paramters can be found on the [dedicated page](https://huggingface.co/docs/transformers/model_doc/deberta#transformers.DebertaConfig), which can be a bit confusing.\n",
    "\n",
    "* learning_rate = 8e-5\n",
    "* batch_size = 16\n",
    "* weight_decay = 0.01\n",
    "* epochs = 1\n",
    "* warmup_ratio = 0.1\n",
    "* cls_dropout = 0.3\n",
    "* lr_scheduler_type = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the default hyperparameters for the rain classifier from Bootcamp\n",
    "parameters = {\n",
    "    \"learning_rate\": 8e-5,\n",
    "    \"batch_size\": 16,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"epochs\": 1,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"cls_dropout\": 0.3,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_config_base = transformers.AutoConfig.from_pretrained(model_nm)\n",
    "os.makedirs(folder_to_output, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = get_trainer(dataset, db_config_base, model_nm, folder_to_output, parameters)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = get_dataset_from_csv(\n",
    "    ds_tweets.loc[indices_test], tok_func, tokenizer, indices_train, indices_test, train=False\n",
    ")\n",
    "ds_test = ds_tweets.loc[indices_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting ROC and Confusin Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.nn.functional.softmax(torch.Tensor(trainer.predict(test_ds).predictions)).numpy()\n",
    "prediction_probability = preds[:, 1]\n",
    "predictions = preds.argmax(axis=-1)\n",
    "truth = ds_test.relevance.values\n",
    "plotting.analysis.classification_report(labels=truth, predictions=predictions)\n",
    "plotting.analysis.plot_roc(truth=truth, prediction_probability=prediction_probability, filename=file + \"_ROC.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.analysis.check_prediction(truth=truth, prediction=predictions, filename=file + \"_CM.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
