+ '[' -z '' ']'+ case "$-" in+ __lmod_vx=x+ '[' -n x ']'+ set +xShell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/p/software/juwelsbooster/lmod/8.7.12/init/bash)Shell debugging restarted+ unset __lmod_vx+ srun apptainer run --nv /p/project/deepacf/maelstrom/haque1/apptainer_images/ap2falcon.sif python3 falcon100Tweet.pyLoading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:51,  6.49s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.53s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:19<00:39,  6.56s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.59s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.64s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:39<00:19,  6.63s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:46<00:13,  6.64s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:53<00:06,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:58<00:00,  6.31s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:58<00:00,  6.51s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.  warnings.warn(The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.Setting `pad_token_id` to `eos_token_id`:11 for open-end generation./usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.  warnings.warn(/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `60` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.  warnings.warn(The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.