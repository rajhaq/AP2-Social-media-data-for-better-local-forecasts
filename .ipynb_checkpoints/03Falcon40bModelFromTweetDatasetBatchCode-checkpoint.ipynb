{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238fac3c-fb1d-42b1-bc93-62e36c5cc819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78a3954-8456-446c-8ea9-9a644e33f2af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2756a4ca1d684f97b3563e3eb4247426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"/p/project/deepacf/maelstrom/ehlert1/models/falcon-40b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, device_map=\"auto\", trust_remote_code=False, quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_prompt(prompt):\n",
    "    return tokenizer.encode(prompt, return_tensors=\"pt\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136e9b6-d461-4691-abb7-1444fdf9a493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Prepare the prompt\n",
    "prompt = r\"\"\"\n",
    "Read below Tweets and tell me if they say that it is raining or sunny. It should be rainy or sunny now.\n",
    "Format your answer in a human readable way,\n",
    "\n",
    "Tweets:\n",
    "Tweet: \"The sound of rain tapping on the window\" \n",
    "Tweet: \"Boris likes drinking water\".\n",
    "Tweet: \"Rain is my imaginary love language, it rains always in my eyes\"\n",
    "\"\"\" \n",
    "\n",
    "example_output = \"\"\"\n",
    "Return the results in a json file like: [ \n",
    "{ \"content\": \"The sound of rain tapping on the window\", \"explanation\": \"The sound of rain heard implies that is raining.\", \"score\": 0.9 },  \n",
    "{ \"content\": \"Boris likes drinking water\", \"explanation\": \"The Tweet does not mention any information related to presence of rain or sun.\", \"score\": 0.1},\n",
    "{ \"content\": ... \n",
    "] \n",
    "\n",
    "Result: [ { \"content\":\"\"\"\n",
    "input_ids = tokenize_prompt(prompt + example_output)\n",
    "sequences = model.generate(\n",
    "    input_ids,\n",
    "    temperature=0.7,\n",
    "    # do_sample=True,\n",
    "    max_length=300,\n",
    "    # top_k=50,\n",
    "    # top_p=0.95,\n",
    "    # num_return_sequences=3\n",
    ")\n",
    "# Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1583e-bc46-4b9a-9116-600d9515959d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, sample_output in enumerate(sequences):\n",
    "    prediction = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    print(f\"{prompt=}\")\n",
    "    print(f\"---------\")\n",
    "    print(f\"prediction\\n{prediction}\")\n",
    "with open(\"dump_relevance.txt\", \"a\") as fd:\n",
    "            fd.write(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3189d620-3fe8-42c1-a075-87dc583e796d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to process tweets in batches\n",
    "def extract_json_array(content): \n",
    "        # Find the starting index of 'Result: '\n",
    "        start_index = content.find('Result: [')\n",
    "        if start_index == -1:\n",
    "            return None  # 'Result: ' not found in the file\n",
    "\n",
    "        # Adjust the start index to the beginning of the JSON array\n",
    "        start_index += len('Result: ')\n",
    "        \n",
    "        # Find the ending index of the JSON array\n",
    "        end_index = content.find(']', start_index)\n",
    "        if end_index == -1:\n",
    "            end_index = len(content) \n",
    "\n",
    "        # Extract the JSON array string\n",
    "        json_array_str = content[start_index:end_index + 1]\n",
    "\n",
    "        return json_array_str\n",
    "\n",
    "def process_tweets(file_path, batch_size=4, max_tweets=10, start_index=610, end_index=620):\n",
    "    # Read the tweets from the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    tweets = df['text'][start_index:end_index].tolist()  # Assuming the column name is 'text'\n",
    "    indices = df['index'][start_index:end_index].tolist()  # Assuming there is a column named 'index'\n",
    "\n",
    "\n",
    "    # Process tweets in batches\n",
    "    all_index = \"\"\n",
    "    for i in range(0,  len(tweets), batch_size):\n",
    "        batch_tweets = tweets[i:i + batch_size]\n",
    "        batch_indices = indices[i:i + batch_size]\n",
    "        prompt = r\"\"\"\n",
    "        Read below Tweets and tell me if they say that it is raining or sunny. It should be rainy or sunny now.\n",
    "        Format your answer in a human readable way,\n",
    "\n",
    "        Tweets:\n",
    "        \"\"\"\n",
    "        for index, tweet in zip(batch_indices, batch_tweets):\n",
    "            prompt += f\"\"\"Tweet: \"{tweet}\"\\n'\n",
    "        \"\"\"\n",
    "            all_index = all_index+f\"\"\"{index},\"\"\"\n",
    "        # The rest of your text generation pipeline code here...\n",
    "        input_ids = tokenize_prompt(prompt + example_output)\n",
    "        sequences = model.generate(\n",
    "            input_ids,\n",
    "            temperature=0.7,\n",
    "            # do_sample=True,\n",
    "            max_length=690,\n",
    "            top_k=50,\n",
    "        )\n",
    "        all_index =all_index+ r\"\"\"\n",
    "        //////////////////////\n",
    "        \"\"\"\n",
    "\n",
    "        # Display and save the results\n",
    "        for sample_output in sequences:\n",
    "            prediction = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "            print(prediction)\n",
    "            prediction = extract_json_array(prediction)\n",
    "            \n",
    "            with open(\"output.txt\", \"a\") as fd:\n",
    "                fd.write(all_index+prediction)\n",
    "            all_index = r\"\"\"\n",
    "            //////////////////////\n",
    "            \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc47c7c-242c-4184-9fa3-3ed6773eb3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_tweets('/p/project/deepacf/maelstrom/haque1/AP2-Social-media-data-for-better-local-forecasts/data/tweets_2017_01_era5_normed_filtered.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap2falcon",
   "language": "python",
   "name": "ap2falcon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
