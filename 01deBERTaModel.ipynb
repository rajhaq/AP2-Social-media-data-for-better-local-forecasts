{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4dd33a3",
   "metadata": {},
   "source": [
    "# Train our Baseline Model (DeBERTa)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we train a classifier based on the [DeBERTa small](https://huggingface.co/microsoft/deberta-v3-small/tree/main) model. This notebook serves as an application following the reference of Bootcamp day 2, 04ModelTraining.ipynb .\n",
    "\n",
    "## Key Steps and Objectives\n",
    "\n",
    "1. **Classifier Training**: We use the DeBERTa small model to train a classifier. The training process involves fine-tuning the model on our specific task or dataset.\n",
    "\n",
    "2. **Results Visualization**: The model correctly predicts rain 58% of the time but incorrectly predicts rain when it's not raining 22% of the time. It's more cautious about predicting no rain, with a true negative rate of 12% and a false negative rate of 7.9%. We visualize the model's performance by generating and examining the following:\n",
    "\n",
    "    - **Confusion Matrix**: A matrix that provides insights into the classifier's ability to correctly classify instances.\n",
    "    \n",
    "    - **ROC Curve**: A Receiver Operating Characteristic curve that illustrates the classifier's performance across different threshold values.\n",
    "    \n",
    "    - **Classifier Certainty**: We assess the certainty of the classifier's predictions, offering insights into its level of confidence in its decisions.\n",
    "\n",
    "\n",
    "This notebook helps us understand how well the DeBERTa-based classifier performs on our task and provides valuable insights through visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146e902-4454-40e1-be0f-bde77704ceb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b1b8e-4c95-4389-99b2-bb7615c948dd",
   "metadata": {},
   "source": [
    "# Library Imports and Directory Setup\n",
    "\n",
    "This section of the code imports necessary libraries and sets up the directory paths for the project. It also includes custom module imports related to plotting and training distribution.\n",
    "\n",
    "Please make sure to replace the relative directory paths ('../helpers/plotting', '../helpers') with actual paths relevant to your project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58ed01-6d5b-4812-8de4-4527d52933ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import sys\n",
    "import transformers\n",
    "import datasets\n",
    "import functools\n",
    "import xarray as xr\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\"../scripts/plotting\")\n",
    "import test_training_distribution\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "from transformer_trainer import get_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1df20c-4b48-44f4-92c1-208a00227446",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset Loading and Label Definition\n",
    "\n",
    "In this section of the code, a dataset is loaded from a specified location, and labels are defined based on a certain condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d873ab1-2636-49f2-a40d-34776b61ec91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_raw = xr.open_dataset(\"/p/project/deepacf/maelstrom/haque1/dataset/tweets_2017_01_era5_normed_filtered.nc\")\n",
    "# again define labels\n",
    "key_tp = \"tp_h\"\n",
    "ds_raw[\"raining\"] = ([\"index\"], ds_raw[key_tp].values > 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa35482-fdcc-413d-920b-e8cb78887276",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Label Definition and Data Splitting\n",
    "\n",
    "In this section of the code, labels are extracted from the loaded dataset, and the dataset is split into training and testing sets while maintaining stratified sampling based on the labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded38aed-ec08-47ed-abde-7fbe4deff2df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = ds_raw[\"raining\"]\n",
    "indices_train, indices_test = train_test_split(ds_raw.index, test_size=0.20, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156405d-d1a5-48ff-bd1b-6f2aadaa1ed7",
   "metadata": {},
   "source": [
    "# Pretrained Tokenizer and Dataset Preparation\n",
    "\n",
    "This section of the code loads a pretrained tokenizer and sets up functions for tokenization. It also prepares the dataset for training and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4d3c9-7ecc-4082-8298-f38b3e14a058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pretrained tokenizer and model configuration\n",
    "model_nm = \"/p/project/deepacf/maelstrom/haque1/deberta-v3-small\"  # Path to model\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_nm)\n",
    "db_config_base = transformers.AutoConfig.from_pretrained(model_nm, num_labels=2)\n",
    "\n",
    "\n",
    "# Define function to tokenize the field 'inputs' stored in x\n",
    "def tok_func(x, tokenizer):\n",
    "    return tokenizer(x[\"inputs\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "# Function to convert the dataset to a format used by Hugging Face\n",
    "def get_dataset(ds, tok_func, tokenizer, indices_train, indices_test, train=True):\n",
    "    df = ds[[\"text_normalized\", \"raining\"]].to_pandas()\n",
    "    df = df.rename(columns={\"text_normalized\": \"inputs\", \"raining\": \"labels\"})\n",
    "    datasets_ds = datasets.Dataset.from_pandas(df)\n",
    "    tok_function_partial = functools.partial(tok_func, tokenizer=tokenizer)\n",
    "    tok_ds = datasets_ds.map(tok_function_partial, batched=True)\n",
    "    if train:\n",
    "        return datasets.DatasetDict({\"train\": tok_ds.select(indices_train), \"test\": tok_ds.select(indices_test)})\n",
    "    else:\n",
    "        return tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe35745-03b8-4f68-b8dc-9aa2eb7a0358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = get_dataset(ds_raw, tok_func, tokenizer, indices_train, indices_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18499b18-a094-4477-a7ba-8307ae41f9a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Output Folder Definition\n",
    "\n",
    "The path to the output folder is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf62831-9db7-4e2e-bd11-2670583ddaae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER_TO_OUTPUT = \"./models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1bb6a-2430-46c6-a021-569197987f83",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Project Parameters and Output Folder Creation\n",
    "\n",
    "In this section of the code, project-specific parameters are defined, and an output folder is created to store project outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a3310-6a32-42fe-b859-da4dd2b7b93b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"learning_rate\": 8e-5,\n",
    "    \"batch_size\": 16,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"epochs\": 1,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"cls_dropout\": 0.3,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "}\n",
    "\n",
    "os.makedirs(FOLDER_TO_OUTPUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f27610-15ec-4d67-8905-215fab48f8e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Training Initialization and Execution\n",
    "\n",
    "In this section of the code, a trainer for the machine learning model is initialized, and the training process is started.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef66da-ec62-4094-a801-fc090434b4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = get_trainer(dataset, db_config_base, model_nm, FOLDER_TO_OUTPUT, parameters)\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf8702-f4f7-4a4f-9255-d1cf88cb549c",
   "metadata": {},
   "source": [
    "# Test Dataset Preparation and Dataset Selection\n",
    "\n",
    "In this section of the code, the test dataset is prepared in the format expected by Hugging Face's Transformers library. Additionally, a subset of the original xarray dataset is selected for further analysis.\n",
    "\n",
    "This section prepares the test dataset and extracts the relevant subset of data for further evaluation and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e8dd9-5c30-4a1e-9dea-542ed74c6b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is the test dataset in the format expected by Hugging Face\n",
    "test_ds = get_dataset(\n",
    "    ds_raw.sel(index=indices_test),\n",
    "    tok_func,\n",
    "    tokenizer,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    "    train=False,  # not training anymore\n",
    ")\n",
    "# this is a selection of our xarray dataset that corresponds to the tweets that are part of the test set\n",
    "ds_test = ds_raw.sel(index=indices_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0bdd15-4bf9-4317-ad53-77ececab01e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Prediction and Evaluation\n",
    "\n",
    "In this section of the code, the trained model is used to make predictions on the test dataset, and various evaluation plots are generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d71e74-b9f6-436e-8bc0-e414d1f15d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../bootcamp/AP2/scripts\")\n",
    "import plotting\n",
    "\n",
    "preds = torch.nn.functional.softmax(torch.Tensor(trainer.predict(test_ds).predictions)).numpy()\n",
    "prediction_probability = preds[:, 1]\n",
    "predictions = preds.argmax(axis=-1)\n",
    "truth = ds_test.raining.values\n",
    "plotting.analysis.classification_report(labels=truth, predictions=predictions)\n",
    "plotting.analysis.plot_roc(truth=truth, prediction_probability=prediction_probability)\n",
    "plotting.plotting.analysis.check_prediction(truth=truth, prediction=predictions);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap2",
   "language": "python",
   "name": "ap2_hf-llm-bnb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
