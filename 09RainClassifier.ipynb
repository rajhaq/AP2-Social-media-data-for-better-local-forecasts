{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10eb7752",
   "metadata": {},
   "source": [
    "## Rain Classifier through Relevance-Filtered Data Selection\n",
    "\n",
    "### Objective\n",
    "The main goal of this project is to develop a rain classifier utilizing relevant Tweets. \"Relevant\" Tweets are those identified by the relevance classifier as containing adequate information. \n",
    "\n",
    "### Key Components\n",
    "1. **Relevance Classifier Integration:** Implement the [07RelevanceClassifierForNewDataset](https://github.com/rajhaq/AP2-Social-media-data-for-better-local-forecasts/blob/4-train-relevance-classifier/07RelevanceClassifierForNewDataset.ipynb) notebook to filter and select \"relevant\" Tweets from the dataset. This step ensures that only informative Tweets are utilized for rain classification.\n",
    "2. **Baseline Model Data:** Utilize a one-year dataset (2017) as the baseline for comparison. The initial dataset will undergo filtration by removing 'snow' related tweets.\n",
    "3. **Rain Classifier Training (First Iteration):** Initially, train the rain classifier using the one-year dataset (2017), similar to the approach followed during the botcamp Day 2.\n",
    "4. **Rain Classifier Training (Second Iteration):** Train the rain classifier exclusively using the subset of Tweets marked as \"relevant\" by the relevance classifier. This step involves retraining the rain classifier to focus solely on informative Tweets.\n",
    "5. **Performance Evaluation:** Evaluate the performance of both iterations of the rain classifier. Compare the results obtained from the baseline model and the models trained on the entire Tweet dataset and the filtered dataset. This comparative analysis will provide insights into the effectiveness of utilizing relevant Tweets for rain classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950d702-cf6c-4362-bb3e-7efa581a80ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import logging\n",
    "import functools\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray\n",
    "\n",
    "# Pytorch modules\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "# scikit-learn modules\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "\n",
    "# \"Hugging Face\" modules\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "sys.path.append(\"/bootcamp/AP2/scripts\")\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3128d4-d037-46d6-ae75-925309b67bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[torch.cuda.device(i) for i in range(torch.cuda.device_count())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b523a",
   "metadata": {},
   "source": [
    "## First Iteration\n",
    "The rain classifier trained on the one-year dataset (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b707c3-d938-481f-b5dc-9231b8d72cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_path = \"/p/project/deepacf/maelstrom/haque1/dataset/\"\n",
    "file_name = folder_path + \"tweets_2017_era5_normed_filtered.nc\"\n",
    "ds_tweets = xarray.load_dataset(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e38d1-66aa-4e5b-91b9-7a3d55cc5e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# again define labels\n",
    "key_tp = \"tp_h\"\n",
    "ds_tweets[\"raining\"] = ([\"index\"], ds_tweets[key_tp].values > 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6c0ca-4a63-4e3f-ad82-a5652629369b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# removing snow related tweets\n",
    "ds_tweets = ds_tweets.where(~ds_tweets.text_normalized.str.contains(\"snow\", flags=re.IGNORECASE), drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dbdf8c-11b0-4189-b3e4-f82e3e7d8447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_train, indices_test = sklearn.model_selection.train_test_split(\n",
    "    np.arange(ds_tweets[\"index\"].shape[0]),\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    test_size=0.2,\n",
    "    stratify=ds_tweets[\"raining\"].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524136f4-efc9-451b-a879-9bfdb26e1df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the pretrained tokenizer\n",
    "model_nm = (\n",
    "    \"/p/project/deepacf/maelstrom/haque1/deberta-v3-small\"  # model repo downloaded from Hugging Face to the cluster\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_nm)\n",
    "db_config_base = transformers.AutoConfig.from_pretrained(model_nm, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c44db-6752-4234-af5f-350a2df0148a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tok_func(x, tokenizer):\n",
    "    \"\"\"\n",
    "    tokenizes the field 'inputs' stored in x including padding\n",
    "    \"\"\"\n",
    "    return tokenizer(x[\"inputs\"], padding=True)\n",
    "\n",
    "\n",
    "def get_dataset(ds, tok_func, tokenizer, indices_train, indices_test, train=True):\n",
    "    \"\"\"\n",
    "    converts dataset to 'dataset' format required by Hugging Face\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    ds: dataset\n",
    "    tok_func: functiond use for tokenization\n",
    "    indices_train: indices corresponding to the training set\n",
    "    indices_test: indices corresponding to the training set\n",
    "    train: if used for training\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    header of file\n",
    "    \"\"\"\n",
    "    # converting dataset to pandas as Hugging Face datasets has inbuilt function that converts pandas dataframe to a Hugging Face dataset\n",
    "    df = ds[[\"text_normalized\", \"raining\"]].to_pandas()\n",
    "    df = df.rename(columns={\"text_normalized\": \"inputs\"})\n",
    "    df = df.rename(columns={\"raining\": \"label\"})\n",
    "    datasets_ds = datasets.Dataset.from_pandas(df)\n",
    "    tok_function_partial = functools.partial(tok_func, tokenizer=tokenizer)\n",
    "    tok_ds = datasets_ds.map(tok_function_partial, batched=True)\n",
    "    if train:\n",
    "        return datasets.DatasetDict({\"train\": tok_ds.select(indices_train), \"test\": tok_ds.select(indices_test)})\n",
    "    else:\n",
    "        return tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cefecd-ed46-4457-a326-5ac21774ebb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Hugging Face 'dataset'\n",
    "dataset = get_dataset(ds_tweets, tok_func, tokenizer, indices_train, indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7844e7d-478c-466d-98e1-2e4915f79fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we define our hyper-parameters in a dictionary `parameters`\n",
    "parameters = {}\n",
    "parameters[\"learning_rate\"] = 8e-5\n",
    "parameters[\"batch_size\"] = 16\n",
    "parameters[\"weight_decay\"] = 0.01\n",
    "parameters[\"epochs\"] = 1\n",
    "parameters[\"warmup_ratio\"] = 0.1\n",
    "parameters[\"cls_dropout\"] = 0.3\n",
    "parameters[\"lr_scheduler_type\"] = \"cosine\"\n",
    "\n",
    "FOLDER_TO_OUTPUT = \"/p/project/deepacf/maelstrom/haque1/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f3a19-111b-4400-af5f-8e044f6e3074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(params, db_config_base, model_nm):\n",
    "    \"\"\"\n",
    "    function to retrieve model, format follows Hugging Face convention (parameter -> 'params')\n",
    "    \"\"\"\n",
    "    db_config = db_config_base\n",
    "    if params is not None:\n",
    "        db_config.update({\"cls_dropout\": params[\"cls_dropout\"]})\n",
    "    db_config.update({\"num_labels\": 2})\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(model_nm, config=db_config)\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    compute f1 metrics of both labels, format follows Hugging Face convention\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    eval_pred: evaluation/test set probalities for classification task\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary returning labeled f1 score of \"not raining\" and \"raining\"\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    classification_report = sklearn.metrics.classification_report(\n",
    "        labels, predictions, target_names=[\"not raining\", \"raining\"], output_dict=True\n",
    "    )\n",
    "    f1_not_raining = classification_report[\"not raining\"][\"f1-score\"]\n",
    "    f1_raining = classification_report[\"raining\"][\"f1-score\"]\n",
    "    return {\"f1_not_raining\": f1_not_raining, \"f1_raining\": f1_raining}\n",
    "\n",
    "\n",
    "def get_trainer(dataset, db_config_base, model_nm, FOLDER_TO_OUTPUT, parameters):\n",
    "    \"\"\"\n",
    "    initializes `transformers.Trainer`, which is used to train models with Hugging Face\n",
    "\n",
    "    Hyper parameters are here assigned to model.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataset: dataset in format required by Hugging Face\n",
    "    db_config_base: default model configurations\n",
    "    model_nm: model folder\n",
    "    FOLDER_TO_OUTPUT: folder where trained model, tokenizer,... will be saved\n",
    "    parameters: dictionary of hyper-parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    trainer with assigned parameters used for training\n",
    "    \"\"\"\n",
    "    args = transformers.TrainingArguments(\n",
    "        FOLDER_TO_OUTPUT,\n",
    "        learning_rate=parameters[\"learning_rate\"],\n",
    "        warmup_ratio=parameters[\"warmup_ratio\"],\n",
    "        lr_scheduler_type=parameters[\"lr_scheduler_type\"],\n",
    "        disable_tqdm=False,\n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=parameters[\"batch_size\"],\n",
    "        per_device_eval_batch_size=parameters[\"batch_size\"],\n",
    "        num_train_epochs=parameters[\"epochs\"],\n",
    "        weight_decay=parameters[\"weight_decay\"],\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    # convert `get_model` to partial function to pass it as an argument in `transformers.Trainer`\n",
    "    # see https://www.geeksforgeeks.org/partial-functions-python/ for quick tutorial\n",
    "    get_model_partial = functools.partial(get_model, db_config_base=db_config_base, model_nm=model_nm)\n",
    "    return transformers.Trainer(\n",
    "        model_init=get_model_partial,\n",
    "        args=args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed1112-99cd-428c-8327-d0cef8faa9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = get_trainer(dataset, db_config_base, model_nm, FOLDER_TO_OUTPUT, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6df48-d0a8-4ad1-a45b-1cc26cb27416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed32b6a-6603-4ed3-825a-5be525cc1859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ds = get_dataset(\n",
    "    ds_tweets.isel(index=indices_test),  # Use isel() instead of sel() for integer indexing\n",
    "    tok_func,\n",
    "    tokenizer,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    "    train=False,  # not training anymore\n",
    ")\n",
    "# this is a selection of our xarray dataset that corresponds to the tweets that are part of the test set\n",
    "ds_test = ds_tweets.isel(index=indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e9db03-25f4-447e-832f-7ee5f434cf7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = torch.nn.functional.softmax(torch.Tensor(trainer.predict(test_ds).predictions)).numpy()\n",
    "prediction_probability = preds[:, 1]\n",
    "predictions = preds.argmax(axis=-1)\n",
    "truth = ds_test.raining.values\n",
    "plotting.analysis.classification_report(labels=truth, predictions=predictions)\n",
    "plotting.analysis.plot_roc(truth=truth, prediction_probability=prediction_probability)\n",
    "plotting.plotting.analysis.check_prediction(truth=truth, prediction=predictions);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb4070",
   "metadata": {},
   "source": [
    "## Second Iteration\n",
    "Rain classifier using only the relevant Tweets identified by the relevance classifier.\n",
    "- Load the relevant classifier.Specify the model path (model_nm) to the desired checkpoint in the output.\n",
    "- Filter the dataset `ds_tweets` by the relevance classifier.\n",
    "- Train the model using the new filtered dataset.\n",
    "- Test the model's performance and plot the ROC curve and confusion matrix. Compare the results with the main classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ecd7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = \"/p/project/deepacf/maelstrom/haque1/AP2-Social-media-data-for-better-local-forecasts/tests/outputs/checkpoint-268\"  # replace with relevance classifier checkpoint folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a830748-2a61-415b-961b-61b026ad3f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer_evaluate = load_saved_trained_model(\n",
    "    ds_tweets,\n",
    "    checkpoint_folder,\n",
    "    db_config_base,\n",
    "    model_nm,\n",
    "    parameters,\n",
    ")\n",
    "# obtain test dataset in Huggin Face format\n",
    "trainer_ds = get_dataset(\n",
    "    ds_tweets,\n",
    "    tok_func,\n",
    "    tokenizer,\n",
    "    [],\n",
    "    ds_tweets[\"index\"],\n",
    "    train=False,  # not training anymore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdcf968-0a0d-45c6-b956-c614f0adf4a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds = torch.nn.functional.softmax(torch.Tensor(trainer_evaluate.predict(trainer_ds).predictions)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423aab54-11d8-458e-af63-7565a759ec33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(preds[:, 1], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdaa834-acea-4002-ae4c-fb19829c0f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_ds_tweets = ds_tweets.sel(index=ds_tweets[\"index\"][preds[:, 1] > 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa9af03-1c02-4baa-9d89-8bbaf349345a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, pred in enumerate(preds):\n",
    "    print(i, pred, pred[1] > 0.6, pred.max())\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4d521-497b-48b3-ae0f-bd1f43a2021a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_indices_train, filtered_indices_test = sklearn.model_selection.train_test_split(\n",
    "    np.arange(filtered_ds_tweets[\"index\"].shape[0]),\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    test_size=0.2,\n",
    "    stratify=filtered_ds_tweets[\"raining\"].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ef2b10-f5e3-42af-b5fa-e1114e6db3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Hugging Face 'dataset'\n",
    "filtered_dataset = get_dataset(filtered_ds_tweets, tok_func, tokenizer, filtered_indices_train, filtered_indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e90f35-54f6-4bfe-b60f-7409b46946d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_trainer = get_trainer(filtered_dataset, db_config_base, model_nm, FOLDER_TO_OUTPUT, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151c6e8-1778-49d3-9765-fa7298f7f69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede79be-b70f-45ac-b4ed-7a2167bb0605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_test_ds = get_dataset(\n",
    "    filtered_ds_tweets.isel(index=filtered_indices_test),  # Use isel() instead of sel() for integer indexing\n",
    "    tok_func,\n",
    "    tokenizer,\n",
    "    filtered_indices_train,\n",
    "    filtered_indices_test,\n",
    "    train=False,  # not training anymore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb412d-3571-48c3-8a37-9663dfa10145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is a selection of our xarray dataset that corresponds to the tweets that are part of the test set\n",
    "filtered_ds_test = filtered_ds_tweets.isel(index=filtered_indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5dfa88-e1a6-4dd7-990b-40a2284aa5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = torch.nn.functional.softmax(torch.Tensor(filtered_trainer.predict(filtered_test_ds).predictions)).numpy()\n",
    "prediction_probability = preds[:, 1]\n",
    "predictions = preds.argmax(axis=-1)\n",
    "truth = filtered_ds_test.raining.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea312a6-2d82-4d6f-aa19-92decd6be264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting.analysis.classification_report(labels=truth, predictions=predictions)\n",
    "plotting.analysis.plot_roc(truth=truth, prediction_probability=prediction_probability)\n",
    "plotting.plotting.analysis.check_prediction(truth=truth, prediction=predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap2",
   "language": "python",
   "name": "ap2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
