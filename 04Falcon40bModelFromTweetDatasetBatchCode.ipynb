{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fac3c-fb1d-42b1-bc93-62e36c5cc819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78a3954-8456-446c-8ea9-9a644e33f2af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"/p/project/deepacf/maelstrom/ehlert1/models/falcon-40b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, device_map=\"auto\", trust_remote_code=False, quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def tokenize_prompt(prompt):\n",
    "    return tokenizer.encode(prompt, return_tensors=\"pt\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a457ee84-2a7f-4bec-85cf-9dbe2c327040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136e9b6-d461-4691-abb7-1444fdf9a493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the prompt\n",
    "prompt = r\"\"\"\n",
    "Read below Tweets and tell me if they say that it is raining or sunny. It should be rainy or sunny now.\n",
    "Format your answer in a human readable way,\n",
    "\n",
    "Tweets:\n",
    "Tweet: \"Alarm went off 45 mins ago and I'm still in bed wondering if there's any #snow outside.\"\n",
    "Tweet: \"@innocent Alas, poor Blackpool... no snow in the North West. Like, ever. ðŸ˜¢\"\n",
    "Tweet: \"@yourdog Alaska drying off after a walk in the rain https://t.co/abTt7RoL9h\"\n",
    "Tweet: \"Alarm is set and I'll be live from 5am on @compassfm! We'll be tackling these floods and snow together! We can do this!!\"\n",
    "\"\"\"\n",
    "\n",
    "example_output = \"\"\"\n",
    "Return the results in a json file like: [ \n",
    "{ \"content\": \"The sound of rain tapping on the window\", \"explanation\": \"The sound of rain heard implies that is raining.\", \"score\": 0.9 },  \n",
    "{ \"content\": \"Boris likes drinking water\", \"explanation\": \"The Tweet does not mention any information related to presence of rain or sun.\", \"score\": 0.1},\n",
    "{ \"content\": ... \n",
    "] \n",
    "\n",
    "Result: [ { \"content\":\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb862ac5-2310-4081-b5f0-2693d3f70e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input_ids = tokenize_prompt(prompt + example_output)\n",
    "# sequences = model.generate(\n",
    "#     input_ids,\n",
    "#     temperature=0.7,\n",
    "#     # do_sample=True,\n",
    "#     max_length=len(prompt + example_output),\n",
    "#     top_k=50,\n",
    "#     # top_p=0.95,\n",
    "#     # num_return_sequences=3\n",
    "# )\n",
    "# # Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1583e-bc46-4b9a-9116-600d9515959d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i, sample_output in enumerate(sequences):\n",
    "#     prediction = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "#     # print(f\"{prompt=}\")\n",
    "#     print(f\"---------\")\n",
    "#     print(f\"prediction\\n{prediction}\")\n",
    "# with open(\"dump_relevance.txt\", \"a\") as fd:\n",
    "#             fd.write(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189d620-3fe8-42c1-a075-87dc583e796d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to process tweets in batches\n",
    "def extract_json_array(content):\n",
    "    # Find the starting index of 'Result: '\n",
    "    start_index = content.find(\"Result: [\")\n",
    "    if start_index == -1:\n",
    "        return None  # 'Result: ' not found in the file\n",
    "\n",
    "    # Adjust the start index to the beginning of the JSON array\n",
    "    start_index += len(\"Result: \")\n",
    "\n",
    "    # Find the ending index of the JSON array\n",
    "    end_index = content.find(\"]\", start_index)\n",
    "    if end_index == -1:\n",
    "        end_index = len(content)\n",
    "\n",
    "    # Extract the JSON array string\n",
    "    json_array_str = content[start_index : end_index + 1]\n",
    "\n",
    "    return json_array_str\n",
    "\n",
    "\n",
    "def process_tweets(file_path, batch_size=10, max_tweets=10, start_index=1777, end_index=1790):\n",
    "    # Read the tweets from the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    tweets = df[\"text\"][start_index:end_index].tolist()  # Assuming the column name is 'text'\n",
    "    indices = df[\"index\"][start_index:end_index].tolist()  # Assuming there is a column named 'index'\n",
    "\n",
    "    # Process tweets in batches\n",
    "    all_index = \"\"\n",
    "    for i in range(0, len(tweets), batch_size):\n",
    "        print(i)\n",
    "        batch_tweets = tweets[i : i + batch_size]\n",
    "        batch_indices = indices[i : i + batch_size]\n",
    "        prompt = r\"\"\"\n",
    "        Read below Tweets and tell me if they say that it is raining or sunny. It should be rainy or sunny now.\n",
    "        Format your answer in a human readable way,\n",
    "\n",
    "        Tweets:\n",
    "        \"\"\"\n",
    "        for index, tweet in zip(batch_indices, batch_tweets):\n",
    "            prompt += f\"\"\"Tweet: \"{tweet}\"\\n'\n",
    "        \"\"\"\n",
    "            all_index = all_index + f\"\"\"{index},\"\"\"\n",
    "        # The rest of your text generation pipeline code here...\n",
    "        input_ids = tokenize_prompt(prompt + example_output)\n",
    "        sequences = model.generate(\n",
    "            input_ids,\n",
    "            temperature=0.7,\n",
    "            # do_sample=True,\n",
    "            max_length=min(len(prompt + example_output + example_output), 2048),\n",
    "            top_k=50,\n",
    "        )\n",
    "        all_index = (\n",
    "            all_index\n",
    "            + r\"\"\"\n",
    "        //////////////////////\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        # Display and save the results\n",
    "        for sample_output in sequences:\n",
    "            prediction = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "\n",
    "            # print(prediction)\n",
    "            print(\n",
    "                r\"\"\"\n",
    "             //////////////////////\n",
    "             \"\"\"\n",
    "            )\n",
    "            print(prediction)\n",
    "            prediction = extract_json_array(prediction)\n",
    "            print(prediction)\n",
    "\n",
    "\n",
    "#             with open(\"output.txt\", \"a\") as fd:\n",
    "#                 fd.write(all_index+prediction)\n",
    "#             all_index = r\"\"\"\n",
    "#             //////////////////////\n",
    "#             \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc47c7c-242c-4184-9fa3-3ed6773eb3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "process_tweets(\n",
    "    \"/p/project/deepacf/maelstrom/haque1/AP2-Social-media-data-for-better-local-forecasts/data/tweets_2017_01_era5_normed_filtered.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd0e960-52d9-4225-b66b-39b3d6b27d42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap2falcon",
   "language": "python",
   "name": "ap2falcon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
